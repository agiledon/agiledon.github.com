<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Architecture | 简单文本]]></title>
  <link href="http://agiledon.github.com/blog/tags/architecture/atom.xml" rel="self"/>
  <link href="http://agiledon.github.com/"/>
  <updated>2013-03-03T15:25:37+08:00</updated>
  <id>http://agiledon.github.com/</id>
  <author>
    <name><![CDATA[张逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[可伸缩系统的架构经验]]></title>
    <link href="http://agiledon.github.com/blog/2013/02/27/scalability-system-architecture-lessons/"/>
    <updated>2013-02-27T11:37:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2013/02/27/scalability-system-architecture-lessons</id>
    <content type="html"><![CDATA[<p>{% img center /images/2013/02/scalability.jpg %}</p>

<p>最近，阅读了Will Larson的文章<a href="http://lethain.com/introduction-to-architecting-systems-for-scale/">Introduction to Architecting System for Scale</a>，感觉很有价值。作者分享了他在Yahoo!与Digg收获的设计可伸缩系统的架构经验。在我过往的架构经验中，由于主要参与开发企业软件系统，这种面向企业内部的软件系统通常不会有太大的负载量，太多的并发量，因而对于系统的可伸缩性考虑较少。大体而言，只要在系统部署上考虑集群以及负载均衡即可。本文给了我很多启发，现把本文的主要内容摘译出来，并结合自己对此的理解。</p>

<!--more-->


<p>Larson首先认为，一个理想的系统，对于容量（Capacity）的增长应该与添加的硬件数是线性的关系。换言之，如果系统只有一台服务器，在增加了另一台同样的机器后，容量应该翻倍。以此类推。这种线性的容量伸缩方式，通常被称之为水平伸缩“Horizontal Scalability”。</p>

<p>在设计一个健壮的系统时，自然必须首要考虑失败的情况。Larson认为，一个理想的系统是当失去其中一台服务器的时候，系统不会崩溃。当然，对应而言，失去一台服务器也会导致容量的响应线性减少。这种情况通常被称为冗余“Redundancy”。</p>

<h3>负载均衡</h3>

<p>无论是水平伸缩还是冗余，都可以通过负载均衡来实现。负载均衡就好似一个协调请求的调停者，它会根据集群中机器的当前负载，合理的分配发往Web服务器的请求，以达到有效利用集群中各台机器资源的目的。显然，这种均衡器应该介于客户端与Web服务器之间，如下图所示：
{% img center /images/2013/02/scalability01.png %}</p>

<p>本文提到了实现负载均衡的几种方法。其一是Smart Client，即将负载均衡的功能添加到数据库（以及缓存或服务）的客户端中。这是一种通过软件来实现负载均衡的方式，它的缺点是方案会比较复杂，不够健壮，也很难被重用（因为协调请求的逻辑会混杂在业务系统中）。对此，Larson在文章以排比的方式连续提出问题，以强化自己对此方案的不认可态度：
{% blockquote %}
Is it attractive because it is the simplest solution? Usually, no. Is it seductive because it is the most robust? Sadly, no. Is it alluring because it'll be easy to reuse? Tragically, no.
{% endblockquote %}</p>

<p>第二种方式是采用硬件负载均衡器，例如<a href="http://www.citrix.com/English/ps2/products/product.asp?contentID=21679">Citrix NetScaler</a>。不过，购买硬件的费用不菲，通常是一些大型公司才会考虑此方案。</p>

<p>如果既不愿意承受Smart Client的痛苦，又不希望花费太多费用去购买硬件，那就可以采用一种混合（Hybird）的方式，称之为软件负载均衡器（Software Load Balancer）。Larson提到了<a href="http://haproxy.1wt.eu/">HAProxy</a>。它会运行在本地，需要负载均衡的服务都会在本地中得到均衡和协调。</p>

<h3>缓存</h3>

<p>为了减轻服务器的负载，还需要引入缓存。文章给出了常见的对缓存的分类，分别包括：预先计算结果（precalculating result，例如针对相关逻辑的前一天的访问量）、预先生成昂贵的索引（pre-generating expensive indexes，例如用户点击历史的推荐）以及在更快的后端存储频繁访问的数据的副本（例如<a href="http://memcached.org/">Memcached</a>）。</p>

<h4>应用缓存</h4>

<p>提供缓存的方式可以分为应用缓存和数据库缓存。此二者各擅胜场。应用缓存通常需要将处理缓存的代码显式地集成到应用代码中。这就有点像使用代理模式来为真实对象提供缓存。首先检查缓存中是否有需要的数据，如果有，就从缓存直接返回，否则再查询数据库。至于哪些值需要放到缓存中呢？有<a href="http://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used">诸多算法</a>，例如根据最近访问的，或者根据访问频率。使用Memcached的代码如下所示：
{% codeblock lang:python %}
key = "user.%s" % user_id
user_blob = memcache.get(key)
if user_blob is None:</p>

<pre><code>user = mysql.query("SELECT * FROM users WHERE user_id=\"%s\"", user_id)
if user:
    memcache.set(key, json.dumps(user))
return user
</code></pre>

<p>else:</p>

<pre><code>return json.loads(user_blob)
</code></pre>

<p>{% endcodeblock %}</p>

<h4>数据库缓存</h4>

<p>数据库缓存对于应用代码没有污染，一些天才的DBA甚至可以在不修改任何代码的情况下，通过数据库调优来改进系统性能。例如通过配置Cassandra行缓存。</p>

<h4>内存缓存</h4>

<p>为了提高性能，缓存通常是存储在内存中。常见的内存缓存包括Memcached和<a href="http://redis.io/">Redis</a>。不过采用这种方式仍然需要合理的权衡。我们不可能一股脑儿的将所有数据都存放在内存中，虽然这会极大地改善性能，但比较起磁盘存储而言，RAM的代价更昂贵，同时还会影响系统的健壮性，因为内存中的数据没有持久化，容易丢失。正如之前提到的，我们应该将需要的数据放入缓存，通常的算法是<a href="http://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used">least recently used</a>，即LRU。</p>

<h4>CDN</h4>

<p>提高性能，降低Web服务器负载的另一种常见做法是将静态媒体放入CDN（Content Distribution Network）中。如下图所示：
{% img center /images/2013/02/scalability02.png %}</p>

<p>CDN可以有效地分担Web服务器的压力，使得应用服务器可以专心致志地处理动态页面；同时，CDN还可以通过地理分布来提高响应请求的性能。在设置了CDN后，当系统接收到请求时，首先会询问CDN以获得请求中需要的静态媒体（通常会通过HTTP Header来配置CDN能够缓存的内容）。如果请求的内容不可用，CDN会查询服务器以获得该文件，并在CDN本地进行缓存，最后再提供给请求者。如果当前网站并不大，引入CDN的效果不明显时，可以考虑暂不使用CDN，在将来可以通过使用一些轻量级的HTTP服务器如<a href="http://nginx.org/">Nginx</a>，为静态媒体分出专门的子域名如static.domain.com来提供服务。</p>

<h4>缓存失效</h4>

<p>引入缓存所带来的问题是如何保证真实数据与缓存数据之间的一致性。这一问题通常被称之为缓存失效（Cache Invalidation）。从高屋建瓴的角度来讲，解决这一问题的办法无非即使更新缓存中的数据。一种做法是直接将新值写入缓存中（通常被称为write-through cache）；另一种做法是简单地删除缓存中的值，在等到下一次读缓存值的时候再生成。</p>

<p>整体而言，要避免缓存实效，可以依赖于数据库缓存，或者为缓存数据添加有效期，又或者在实现应用程序逻辑时，尽量考虑避免此问题。例如不直接使用DELETE FROM a WHERE…来删除数据，而是先查询符合条件的数据，再使得缓存中对应的数据失效，继而根据其主键显式地删除这些行。</p>

<h3>Off-Line处理</h3>

<p>这篇文章还提到了Off-Line的处理方式，即通过引入消息队列的方式来处理请求。事实上，在大多数企业软件系统中，这种方式也是较为常见的做法。在我撰写的文章《<a href="http://agiledon.github.com/blog/2012/12/27/distributed-architecture-based-on-message/">案例分析:基于消息的分布式架构</a>》中，较为详细地介绍了这种架构。在引入消息队列后，Web服务器会充当消息的发布者，而在消息队列的另一端可以根据需要提供消费者Consumer。如下图所示。对于Off-Line的任务是否执行完毕，通常可以通过轮询或回调的方式来获知。
{% img center /images/2013/02/scalability03.png %}</p>

<p>为了更好地提高代码可读性，可以在公开的接口定义中明确地标示该任务是On-Line还是Off-Line。</p>

<p>引入Message Queue，可以极大地缓解Web服务器的压力，因为它可以将耗时较长的任务转到专门的机器上去执行。</p>

<p>此外，通过引入定时任务，也可以有效地利用Web服务器的空闲时间来处理后台任务。例如，通过Spring Batch Job来执行每日、每周或者每月的定时任务。如果需要多台机器去执行这些定时任务，可以引入Spring提供的<a href="https://puppetlabs.com">Puppet</a>来管理这些服务器。Puppet提供了可读性强的声明性语言来完成对机器的配置。</p>

<h4>Map-Reduce</h4>

<p>对于大数据的处理，自然可以引入Map-Reduce。为整个系统专门引入一个Map-Reduce层来处理数据是有必要的。相对于使用SQL数据库作为数据中心的方式，Map-Reduce对可伸缩性的支持更好。Map-Reduce可以与任务的定时机制结合起来。如下图所示：
{% img center /images/2013/02/scalability04.png %}</p>

<h3>平台层</h3>

<p>Larson认为，大多数系统都是Web应用直接与数据库通信，但如果能加入一个平台层（Platform Layer），或许会更好。
{% img center /images/2013/02/scalability05.png %}</p>

<p>首先，将平台与Web应用分离，使得它们可以独立地进行伸缩。例如需要添加一个新的API，就可以添加新的平台服务器，而无需增加Web服务器。要知道，在这样一个独立的物理分层架构中，不同层次对服务器的要求是不一样的。例如，对于数据库服务器而言，由于需要频繁地对磁盘进行I/O操作，因此应保证数据库服务器的IO性能，如尽量使用固态硬盘。而对于Web服务器而言，则对CPU的要求比较高，尽可能采用多核CPU。</p>

<p>其次，增加一个额外的平台层，可以有效地提高系统的可重用性。例如我们可以将一些与系统共有特性以及横切关注点的内容（如对缓存的支持，对数据库的访问等功能）抽取到平台层中，作为整个系统的基础设施（Infrastructure）。尤其对于产品线系统而言，这种架构可以更好地为多产品提供服务。</p>

<p>最后，这种架构也可能对跨团队开发带来好处。平台可以抽离出一些与产品无关的接口，从而隐藏其具体实现的细节。如果划分合理，并能设计出相对稳定的接口，就可以使得各个团队可以并行开发。例如可以专门成立平台团队，致力于对平台的实现以及优化。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS的架构]]></title>
    <link href="http://agiledon.github.com/blog/2013/02/16/architecture-of-hdfs/"/>
    <updated>2013-02-16T15:33:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2013/02/16/architecture-of-hdfs</id>
    <content type="html"><![CDATA[<p>{% img left /images/2013/02/hdfs.jpg %}
HDFS(Hadoop Distributed File System)作为<a href="http://hadoop.apache.org">Hadoop</a>下的一个子项目，是目前使用极为广泛的分布式文件系统。它的设计目的是提供一个高容错，且能部署在廉价硬件的分布式系统；同时，它能支持高吞吐量，适合大规模数据集应用。这一目标可以看做是HDFS的架构目标。显然，这样的架构设计主要还是满足系统的质量属性，包括如何保证分布式存储的可靠性，如何很好地支持硬件的水平扩展，如何支持对大数据处理的高性能以及客户端请求的高吞吐量。所以，HDFS的架构设计颇有参考价值，在Hadoop的Apache官方网站上也给出了<a href="http://hadoop.apache.org/docs/current/hdfs_design.html">HDFS的架构指南</a>。在<a href="http://www.aosabook.org/en/index.html">The Architecture of Open Source Applications</a>卷I的第8章也详细介绍了HDFS的架构。</p>

<p>HDFS的高层设计看起来很简单，主要包含NameNode与DataNode，它们之间的通信，包括客户端与HDFS NameNode服务器的通信则基于TCP/IP。客户端通过一个可配置的TCP端口连接到NameNode，通过ClientProtocol协议与NameNode交互。而DataNode使用DatanodeProtocol协议与NameNode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。</p>

<!--more-->


<p>通常，一个HDFS Cluter由一个NameNode和多个DataNode组成，且在大多数情况下，会由一台专门的机器运行NameNode实例。下图是HDFS的High Level Architecture：
{% img center /images/2013/02/hdfs01.gif %}</p>

<div align="center">本图来自<a href="http://www.ibm.com/developerworks/library/wa-introhdfs/">IBM DeveloperWorks</a></div>


<p>注意，在这个架构图中，观察各节点之间的通信，容易造成一个误解是NameNode会直接与DataNode通信。实则不然。虽然，NameNode可以看做是DataNode的管理者甚至是仲裁者，但由于DataNode的数量通常很多，且都是分布式部署在不同的机器上，若NameNode需要主动发起对各个DataNode的请求，会导致NameNode的负载过大，且对于网络的要求也极高。因此，在设计上，NameNode不会主动发起RPC，而是响应来自客户端或Datanode的RPC请求。如果NameNode需要获得指定DataNode的信息，则是通过DataNode调用函数后的一个简单返回值。每个DataNode都会维护一个开放的Socket，以支持客户端代码或其他DataNode的读写请求。NameNode知道该Socket的Host与Port。</p>

<p>{% img left /images/2013/02/hdfs02.png %}
一个好的架构必然遵循了好的架构原则。HDFS架构有许多值得我们借鉴或参考的设计决策，其中它所遵循的架构原则，对HDFS满足架构目标起到了决定性的作用。这些原则包括：元数据与数据分离；主/从架构；一次写入多次读取；移动计算比移动数据更划算。</p>

<h4>元数据与数据分离</h4>

<p>这主要体现在NameNode与DataNode之分，这种分离是HDFS最关键的架构决策。这两种节点的分离，意味着关注点的分离。对于一个文件系统而言，文件本身的属性（即元数据）与文件所持有的数据属于两个不同的关注点。一个简单的例子是文件名的更改。如果不实现分离，针对一个属性的修改，就可能需要对数据块进行操作，这是不合理的。如果不分离这两种节点，也不利于文件系统的分布式部署，因为我们很难找到一个主入口点。显然，这一原则是与后面提到的主/从架构是一脉相承的。</p>

<p>NameNode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被NameNode记录下来。NameNode会负责执行与文件系统命名空间的操作，包括打开、关闭、重命名文件或目录。它同时还要负责决定数据块到DataNode的映射。从某种意义上讲，NameNode是所有HDFS元数据的仲裁者和资源库。</p>

<p>DataNode则负责响应文件系统客户端发出的读写请求，同时还将在NameNode的指导下负责执行数据库的创建、删除以及复制。</p>

<p>因为所有的用户数据都存放在DataNode中，而不会流过NameNode，就使得NameNode的负载变小，且更有利于为NameNode建立副本。</p>

<h4>主/从架构</h4>

<p>主从架构表现的是Component之间的关系，即由主组件控制从组件。在HDFS中，一个HDFS集群是由一个NameNode和一定数目的DataNode组成。NameNode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的DataNode一般是一个节点一个，负责管理它所在节点上的存储。</p>

<h4>一次写入多次读写</h4>

<p>一次写入多次读写，即Write Once Read Many，是HDFS针对文件访问采取的访问模型。HDFS中的文件只能写一次，且在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。这种模型可以有效地保证数据一致性，且避免了复杂的并发同步处理，很好地支持了对数据访问的高吞吐量。</p>

<h4>移动计算比移动数据更划算</h4>

<p>移动计算比移动数据更划算，即moving computation is cheaper than moving data。对于数据运算而言，越靠近数据，执行运算的性能就越好，尤其是当数据量非常大的时候，更是如此。由于分布式文件系统的数据并不一定存储在一台机器上，就使得运算的数据常常与执行运算的位置不相同。如果直接去远程访问数据，可能需要发起多次网络请求，且传输数据的成本也相当客观。因此最好的方式是保证数据与运算离得最近。这就带来两种不同的策略。一种是移动数据，另一种是移动运算。显然，移动数据，尤其是大数据的成本非常之高。要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据。</p>

<p>HDFS在改善吞吐量与数据访问性能上还做出了一个好的设计决策，就是数据块的Staging。当客户端创建文件时，并没有立即将其发送给NameNode，而是将文件数据存储到本地的临时文件中。这个操作是透明的，客户端不会觉察，也不必关心。文件的创建事实上是一个流数据的写，当临时文件累计的数据量超过一个数据块大小时，客户端才会联系NameNode。NameNode将文件名插入文件系统的层次结构中，并且分配一个数据块给它。然后返回Datanode的标识符和目标数据块给客户端。接着，客户端将这块数据从本地临时文件上传到指定的Datanode上。当文件关闭时，在临时文件中剩余的没有上传的数据也会传输到指定的Datanode上。然后客户端告诉Namenode文件已经关闭。此时Namenode才将文件创建操作提交到HDFS的文件系统。这个操作的大致时序图如下所示：
{% img center /images/2013/02/hdfs03.png %}</p>

<p>采用这种客户端缓存的方式，可以有效地减少网络请求，避免大数据的写入造成网络堵塞，进而提高网络吞吐量。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[软件开发未必等同于盖房子]]></title>
    <link href="http://agiledon.github.com/blog/2013/02/07/comments-on-why-we-should-build-software/"/>
    <updated>2013-02-07T15:13:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2013/02/07/comments-on-why-we-should-build-software</id>
    <content type="html"><![CDATA[<p>{% img center /images/2013/02/sketch.jpg %}
微软研究院首席研究员Leslie Lamport发表了文章<a href="http://www.wired.com.edgesuite.net/opinion/2013/01/code-bugs-programming-why-we-need-specs/#more-122883">Why We Should Build Software Like We Build Houses</a>，吐槽了对如今程序员不愿意做分析，画草图，而是直接开始编码的现状。看了这篇文章后，我对Lamport的观点有一些想法，觉得不吐不快。</p>

<p>其实从文章标题以及作者开篇名义提出的问题来看，显然基于一个假设，或者说事先设定的隐喻，那就是用建筑业来形容软件开发。作者认为建筑设计师在修建房屋之前都会绘制一幅详尽的计划（或蓝图），而软件开发人员却并不这样。以两个不同的行业做对比，认为一个行业这么做了，另一个行业不这样做就有问题，这个假设合理吗？虽然，软件行业中所谓Architecture以及Build的概念确乎来自于建筑行业，甚至这种隐喻在许多年前为诸多大师认可，因而提出诸如软件工程等思想；虽然，它山之石可以攻玉，借鉴别的领域的最佳实践，确乎可以帮助软件开发收获灵感，避免去走太多弯路；然而，毕竟二者之间并不能完全划等号。</p>

<!--more-->


<p>作者似乎看到了这一点，担心这一理论站不住脚，于是在文中驳斥了他自己代表其他程序员提出的问题：“They think tearing down walls is hard but changing code is easy, so blueprints of programs aren’t necessary.”以此来说明，既然修改代码比推倒一堵墙要难，那么修建房子尚且要画蓝图，为何编写代码就不画蓝图呢？看起来，这一论断是合乎逻辑性的，但我始终觉得作者一直在混淆Design与Coding这两个概念。</p>

<p>确如作者所说，许多程序员在Coding的时候，并未做太多分析以及画草图的工作，但他似乎忽略了，更多的程序员在Coding之前，其实还经历了大量的Design工作。这个Design工作与Lamport所谓的绘制草图，有何区别呢？即使采用TDD的做法，通常的做法仍然是需要运用分解任务的方式，来分析需求，理清设计思路，以辨别或识别出领域概念，进而合理地分配职责。就我个人而言，很多时候，我也会对领域模型画一些粗略的类图或时序图；而在开发期间，我们也会就软件开发撰写一些文档，并放在团队wiki上共享出来。</p>

<p>这是让我对本文产生疑惑的地方——那就是作者妄图批判的开发软件的做法其实根本算是一种子虚乌有。</p>

<p>我猜测，作者真正想表达的意思是，因为有了Specification，就能更好地理解设计意图，在将来代码产生变化时，也能够参考此文档，以便于更好地修改代码。这一观点并没有错误，但软件业的开发者不是一直这样践行着吗？多数程序员对文档的诟病是：如何同步文档，使得文档表达的内容能够真实反映程序的实现。对这个问题，作者避而不答。然而，这个问题恰恰是建筑业与软件业一个主要的区别。整体而言，软件业更多地是一种演进而迭代的过程，而世界上大多数建筑（不排除有个别例外，但显然这对于软件业而言，却是常态），在建筑设计师完成设计后，不会做出太多的改变。</p>

<p>正是因为文档的这些问题，才有人提出代码即文档，从而开始推动代码的可读性。当然，也是为了更好地应对变化，才会要求代码具有可扩展性。即使如此，也从来不会有人去彻底地否定文档，尤其针对极为复杂的软件系统而言。</p>

<p>因而，我并不觉得这篇文章有何价值。软件业的最大问题并非从业人员不去编写Specification，多数还是沟通交流的问题，如何正确地理解需求，如何正确地理解设计，如何快速地发布可工作的软件，以期得到用户真正满足其内心需求的反馈。更多的问题还包括诸如管理问题，技术难题，部署问题等等。Specification编写的问题或许存在，需要解决的优先级并没有如此之高。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对CQRS的基础理解]]></title>
    <link href="http://agiledon.github.com/blog/2012/12/31/basic-understanding-on-cqrs/"/>
    <updated>2012-12-31T10:59:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2012/12/31/basic-understanding-on-cqrs</id>
    <content type="html"><![CDATA[<p>{% img center /images/2012/12/seperated.jpg 500 375 %}
CQRS由Greg Young提出，目前在DDD领域中被广泛使用。在我看来，它甚至可以被称为是一种架构风格，可以取得与MapReduce，REST同等的地位，对软件系统的整体架构产生重要影响。</p>

<p>CQRS即Command Query Responsibility Seperation（命令查询职责分离），其设计思想来源于Mayer提出的CQS（Command Query Seperation）。这种命令与查询的分离方式，可以更好地控制请求者的操作。查询操作不会造成数据的修改，因而它属于一种幂等操作，可以反复地发起，而不用担心会对系统造成影响。基于这种特性，我们还可以为其提供缓存，从而改进查询的性能。命令操作则与之相反，它会直接影响系统信息的改变。查询操作与命令操作对事务的要求也不一样。由于查询操作不会改变系统状态，因而，不会产生最终的数据不一致。从请求响应的角度来看，查询操作常常需要同步请求，实时返回结果；命令操作则不然，因为我们并不期待命令操作必须返回结果，这就可以采用fire-and-forget方式，而这种方式正是运用异步操作的前提。此外，对于大多数软件系统而言，查询操作发起的频率通常要远远高于命令操作。如上种种，都是将命令与查询进行分离的根本原因。</p>

<p>这就很好地阐释了我们为何需要运用CQRS模式，同时也说明了CQRS的适用场景。</p>

<!--more-->


<p>只要充分理解了运用CQRS模式的意图，理解CQRS模式就变得容易了许多。下图是CQRS框架<a href="http://www.axonframework.org">AxonFramework</a>官方文档给出的CQRS架构图。
{% img center /images/2012/12/cqrs-arch.png 613 434 %}</p>

<p>在这个架构图中，最核心的概念是Command、Event。以我的理解，CQRS模式的风格源头就是基于事件的异步状态机模型。抛开命令查询分离这一核心原则，这才是CQRS的基础内容。CQRS对设计者的影响，是将领域逻辑，尤其是业务流程，皆看做是一种领域对象状态迁移的过程。这一点与REST将HTTP应用协议看做是应用状态迁移的引擎，有着异曲同工之妙。这种观点（或设计视图）引出了Command与Event的概念。Command是系统中会引起状态变化的活动，通常是一种命令语气，例如注册会议RegisterToConference。至于Event，则描述了某种事件的发生，通常是命令的结果（但并不一定是直接结果，但源头一定是因为发送了命令），例如OrderConfirmed。我发现，这种事件更接近于一种事实，即某次数据改变的结果，是一种确定无疑已经发生的事实。这一思想直接引入了Event Source，并带来Audit（审计）的好处。而它更是与<a href="http://www.datomic.com">Datomic数据库</a>的设计哲学一脉相承。Datomic的设计哲学就是：“将数据(Data)看做是事实(Fact)。每个事实都是过去的痕迹，虽然这种过去可以遗忘，但却无法改变。”Event Source可以将这些事件的发生过程记录下来，使得我们可以追溯业务流程。
{% img center /images/2012/12/datomic.jpg 563 274 %}</p>

<p>Command和Event都有对应的Handler来处理。它们具有一个共同的特征，即支持异步处理方式。这也是为何在架构中需要引入Command Bus和Event Bus的原因。在UI端执行命令请求，事实上就是将命令（注意，这是一个命令对象，你完全将其理解为Command模式的运用。注意，命令的命名一定要恰如其分地体现业务的意图）发送到Command Bus中。Command Bus更像是一个调停者（Mediator），在接收到Command时，会将其路由到准确的CommandHandler，由CommandHandler来处理该命令。在Axon Framework中，Command Bus提供了dispatch()方法对命令进行分发。也就是说，在它的实现中，并没有对Command提供异步处理，而仅仅是完成路由的功能。不过，在我自己的框架实现中，我却将Command Bus看做是消息通道，而将Command Handler看做是该消息通道的侦听者。因此，我引入了队列来实现Command Bus。很难说明哪种方式更合理，这还要取决于业务模型。整体来看，后一个方案似乎有些重型了。不过，我现在并未引入消息队列，而是使用了Scala的Actor，从多线程的角度来实现CommandBus的异步模型，也可以说得过去。</p>

<p>Event的处理与之相似。Axon Framework同时支持同步和异步方式。从框架角度讲，提供更多的选择是一件好事。但基于CQRS模式的核心思想来看，如果对Command(包括Event)的处理未采用异步模型，它就没有发挥出足够的优势，此时采用CQRS，反而会增加设计难度，有些得不偿失。</p>

<p>在Command端，基本的处理流程是由UI发起命令请求，发送到CommandBus，并由它分发给对应的Command Handler来处理命令。Command Handler会与领域对象，特别是与Aggregation Root对象通信。在处理了相关的业务逻辑后，会触发Event。一方面，它会将Event放到Event Store中；另一方面，同时会将Event发送到Event Bus，再由Event Handler处理事件。根据Axon Framework的官方文档，Event Handler会负责更新数据源，从而保证查询端能够得到最新的数据。</p>

<p>然而，这一过程并不是这样简单。因为整个过程可能体现的是一个状态机。Command会导致状态的迁移，并在执行Aggregate的逻辑时，触发对应的Event。Event Handler在处理事件时，并不一定是这个业务过程的终点，它可能会发送引起下一个状态迁移的命令，从而形成一个不断迁移的过程，直至业务完全结束。这就需要我们在引入CQRS时，需要改变之前的设计思路，尽量从状态迁移的角度去理解业务逻辑。UML中的状态图是一个很好的分析工具。另外，它也带来一个挑战，就是事务。因为整个过程都涉及到数据状态的变化，当某个状态迁移出现问题时，要保证数据的最终结果是一致的。Axon Framework的解决方案是引入<a href="http://martinfowler.com/eaaCatalog/unitOfWork.html">Unit of Work模式</a>。此外，在真正实现时，究竟是由Event Handler去更新数据源，还是交由Aggregate去完成，还有待考量。我倾向于由Aggregate委派给Repository来完成。从职责分配的角度来看，这种方式更为合理。因为与数据源打交道的逻辑绝对不能太过于分散，以免数据源的改变影响到整个领域层。在DDD中，持久逻辑都是被封装到Repository（在其内部，又会委派给基础设施层中提供数据访问的对象）。换言之，这种实践是符合DDD的设计思想的。</p>

<p>本文没有详细讨论Repository、Aggregate Root在引入CQRS后，二者会产生什么样的变化？此外，命令查询职责分离模式对数据源的要求，在Command端与Query端的范式已经产生了根本的区别，在设计时我们应该如何考虑？这些内容，我将在以后的博客深入分析。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[案例分析:基于消息的分布式架构]]></title>
    <link href="http://agiledon.github.com/blog/2012/12/27/distributed-architecture-based-on-message/"/>
    <updated>2012-12-27T11:31:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2012/12/27/distributed-architecture-based-on-message</id>
    <content type="html"><![CDATA[<p>{% img center /images/2012/12/messagearchitecture.jpg 600 450 %}</p>

<p>美国计算机科学家，LaTex的作者Leslie Lamport说：“分布式系统就是这样一个系统，系统中一个你甚至都不知道的计算机出了故障，却可能导致你自己的计算机不可用。”一语道破了开发分布式系统的玄机，那就是它的复杂与不可控。所以Martin Fowler强调：分布式调用的第一原则就是不要分布式。这句话看似颇具哲理，然而就企业应用系统而言，只要整个系统在不停地演化，并有多个子系统共同存在时，这条原则就会被迫打破。盖因为在当今的企业应用系统中，很难寻找到完全不需要分布式调用的场景。Martin Fowler提出的这条原则，一方面是希望设计者能够审慎地对待分布式调用，另一方面却也是分布式系统自身存在的缺陷所致。无论是CORBA，还是EJB 2；无论是RPC平台，还是Web Service，都因为驻留在不同进程空间的分布式组件，而引入额外的复杂度，并可能对系统的效率、可靠性、可预测性等诸多方面带来负面的影响。</p>

<p>然而，不可否认的是在企业应用系统领域，我们总是会面对不同系统之间的通信、集成与整合，尤其当面临异构系统时，这种分布式的调用与通信变得越重要，它在架构设计中就更加凸显其价值。并且，从业务分析与架构质量的角度来讲，我们也希望在系统架构中尽可能地形成对服务的重用，通过独立运行在进程中服务的形式，彻底解除客户端与服务端的耦合。这常常是架构演化的必然道路。在我的同事陈金洲发表在InfoQ上的文章《<a href="%E6%9E%B6%E6%9E%84%E8%85%90%E5%8C%96%E4%B9%8B%E8%B0%9C">架构腐化之谜</a>》中，就认为可以通过“将独立的模块放入独立的进程”来解决架构因为代码规模变大而腐化的问题。</p>

<!--more-->


<p>随着网络基础设施的逐步成熟，从RPC进化到Web Service，并在业界开始普遍推行SOA，再到后来的RESTful平台以及云计算中的PaaS与SaaS概念的推广，分布式架构在企业应用中开始呈现出不同的风貌，然而殊途同归，这些分布式架构的目标仍然是希望回到建造巴别塔的时代，系统之间的交流不再为不同语言与平台的隔阂而产生障碍。正如Martin Fowler在《企业集成模式》一书的序中写道：“集成之所以重要是因为相互独立的应用是没有生命力的。我们需要一种技术能将在设计时并未考虑互操作的应用集成起来，打破它们之间的隔阂，获得比单个应用更多的效益”。这或许是分布式架构存在的主要意义。</p>

<h2>1 集成模式中的消息模式</h2>

<p>归根结底，企业应用系统就是对数据的处理，而对于一个拥有多个子系统的企业应用系统而言，它的基础支撑无疑就是对消息的处理。与对象不同，消息本质上是一种数据结构（当然，对象也可以看做是一种特殊的消息），它包含消费者与服务双方都能识别的数据，这些数据需要在不同的进程（机器）之间进行传递，并可能会被多个完全不同的客户端消费。在众多分布式技术中，消息传递相较文件传递与远程过程调用（RPC）而言，似乎更胜一筹，因为它具有更好的平台无关性，并能够很好地支持并发与异步调用。对于Web Service与RESTful而言，则可以看做是消息传递技术的一种衍生或封装。在《面向模式的软件架构（卷四）》一书中，将关于消息传递的模式划归为分布式基础设施的范畴，这是因为诸多消息中间件产品的出现，使得原来需要开发人员自己实现的功能，已经可以直接重用。这极大地降低了包括设计成本、实现成本在内的开发成本。因此，对于架构师的要求也就从原来的设计实现，转变为对业务场景和功能需求的判断，从而能够正确地进行架构决策、技术选型与模式运用。</p>

<h3>常用的消息模式</h3>

<p>在我参与过的所有企业应用系统中，无一例外地都采用（或在某些子系统与模块中部分采用）了基于消息的分布式架构。但是不同之处在于，让我们做出架构决策的证据却迥然而异，这也直接影响我们所要应用的消息模式。</p>

<h4>消息通道（Message Channel）模式</h4>

<p>我们常常运用的消息模式是Message Channel（消息通道）模式，如图1所示。
{% img center /images/2012/12/message1.png 600 264 %}</p>

<center>图1 Message Channel模式（图片来自[eaipatterns](http://www.eaipatterns.com) ）</center>


<p>消息通道作为在客户端（消费者，Consumer）与服务（生产者，Producer）之间引入的间接层，可以有效地解除二者之间的耦合。只要实现规定双方需要通信的消息格式，以及处理消息的机制与时机，就可以做到消费者对生产者的“无知”。事实上，该模式可以支持多个生产者与消费者。例如，我们可以让多个生产者向消息通道发送消息，因为消费者对生产者的无知性，它不必考虑究竟是哪个生产者发来的消息。</p>

<p>虽然消息通道解除了生产者与消费者之间的耦合，使得我们可以任意地对生产者与消费者进行扩展，但它又同时引入了各自对消息通道的依赖，因为它们必须知道通道资源的位置。要解除这种对通道的依赖，可以考虑引入Lookup服务来查找该通道资源。例如，在JMS中就可以通过JNDI来获取消息通道Queue。若要做到充分的灵活性，可以将与通道相关的信息存储到配置文件中，Lookup服务首先通过读取配置文件来获得通道。</p>

<p>消息通道通常以队列的形式存在，这种先进先出的数据结构无疑最为适合这种处理消息的场景。微软的MSMQ、IBM MQ、JBoss MQ以及开源的<a href="RabbitMQ">RabbitMQ</a>、<a href="http://activemq.apache.org">Apache ActiveMQ</a>都通过队列实现了Message Channel模式。因此，在选择运用Message Channel模式时，更多地是要从质量属性的层面对各种实现了该模式的产品进行全方位的分析与权衡。例如，消息通道对并发的支持以及在性能上的表现；消息通道是否充分地考虑了错误处理；对消息安全的支持；以及关于消息持久化、灾备（fail over）与集群等方面的支持。因为通道传递的消息往往是一些重要的业务数据，一旦通道成为故障点或安全性的突破点，对系统就会造成灾难性的影响。在本文的第二部分，我将给出一个实际案例来阐释在进行架构决策时应该考虑的架构因素，并由此做出正确地决策。</p>

<h4>发布者-订阅者（Publisher-Subscriber）模式</h4>

<p>一旦消息通道需要支持多个消费者时，就可能面临两种模型的选择：拉模型与推模型。拉模型是由消息的消费者发起的，主动权把握在消费者手中，它会根据自己的情况对生产者发起调用。如图2所示：
{% img center /images/2012/12/message2.png 600 207 %}</p>

<center>图2  拉模型</center>


<p>拉模型的另一种体现则由生产者在状态发生变更时，通知消费者其状态发生了改变。但得到通知的消费者却会以回调方式，通过调用传递过来的消费者对象获取更多细节消息。</p>

<p>在基于消息的分布式系统中，拉模型的消费者通常以Batch Job的形式，根据事先设定的时间间隔，定期侦听通道的情况。一旦发现有消息传递进来，就会转而将消息传递给真正的处理器（也可以看做是消费者）处理消息，执行相关的业务。在本文第二部分介绍的医疗卫生系统，正是通过引入Quartz.NET实现了Batch Job，完成对消息通道中消息的处理。</p>

<p>推模型的主动权常常掌握在生产者手中，消费者被动地等待生产者发出的通知，这就要求生产者必须了解消费者的相关信息。如图3所示：
{% img center /images/2012/12/message3.png 600 187 %}</p>

<center>图3 推模型</center>


<p>对于推模型而言，消费者无需了解生产者。在生产者通知消费者时，传递的往往是消息（或事件），而非生产者自身。同时，生产者还可以根据不同的情况，注册不同的消费者，又或者在封装的通知逻辑中，根据不同的状态变化，通知不同的消费者。</p>

<p>两种模型各有优势。拉模型的好处在于可以进一步解除消费者对通道的依赖，通过后台任务去定期访问消息通道。坏处是需要引入一个单独的服务进程，以Schedule形式执行。而对于推模型而言，消息通道事实上会作为消费者观察的主体，一旦发现消息进入，就会通知消费者执行对消息的处理。无论推模型，拉模型，对于消息对象而言，都可能采用类似Observer模式的机制，实现消费者对生产者的订阅，因此这种机制通常又被称为Publisher-Subscriber模式，如图4所示：
{% img center /images/2012/12/message4.png 600 393 %}</p>

<center>图4 Publisher-Subscriber模式（图片来自[eaipatterns](http://www.eaipatterns.com) ）</center>


<p>通常情况下，发布者和订阅者都会被注册到用于传播变更的基础设施（即消息通道）上。发布者会主动地了解消息通道，使其能够将消息发送到通道中；消息通道一旦接收到消息，会主动地调用注册在通道中的订阅者，进而完成对消息内容的消费。</p>

<p>对于订阅者而言，有两种处理消息的方式。一种是广播机制，这时消息通道中的消息在出列的同时，还需要复制消息对象，将消息传递给多个订阅者。例如，有多个子系统都需要获取从CRM系统传来的客户信息，并根据传递过来的客户信息，进行相应的处理。此时的消息通道又被称为Propagation通道。另一种方式则属于抢占机制，它遵循同步方式，在同一时间只能有一个订阅者能够处理该消息。实现Publisher-Subscriber模式的消息通道会选择当前空闲的唯一订阅者，并将消息出列，并传递给订阅者的消息处理方法。</p>

<p>目前，有许多消息中间件都能够很好地支持Publisher-Subscriber模式，例如JMS接口规约中对于Topic对象提供的MessagePublisher与MessageSubscriber接口。RabbitMQ也提供了自己对该模式的实现。微软的MSMQ虽然引入了事件机制，可以在队列收到消息时触发事件，通知订阅者。但它并非严格意义上的Publisher-Subscriber模式实现。由微软MVP <a href="http://www.udidahan.com">Udi Dahan</a>作为主要贡献者的<a href="http://nservicebus.com">NServiceBus</a>，则对MSMQ以及WCF做了进一层包装，并能够很好地实现这一模式。</p>

<h4>消息路由（Message Router）模式</h4>

<p>无论是Message Channel模式，还是Publisher-Subscriber模式，队列在其中都扮演了举足轻重的角色。然而，在企业应用系统中，当系统变得越来越复杂时，对性能的要求也会越来越高，此时对于系统而言，可能就需要支持同时部署多个队列，并可能要求分布式部署不同的队列。这些队列可以根据定义接收不同的消息，例如订单处理的消息，日志信息，查询任务消息等。这时，对于消息的生产者和消费者而言，并不适宜承担决定消息传递路径的职责。事实上，根据S单一职责原则，这种职责分配也是不合理的，它既不利于业务逻辑的重用，也会造成生产者、消费者与消息队列之间的耦合，从而影响系统的扩展。</p>

<p>既然这三种对象（组件）都不宜承担这样的职责，就有必要引入一个新的对象专门负责传递路径选择的功能，这就是所谓的Message Router模式，如图5所示：
{% img center /images/2012/12/message5.png 600 175 %}</p>

<center>图5 Message Router模式（图片来自eaipatterns ）</center>


<p>通过消息路由，我们可以配置路由规则指定消息传递的路径，以及指定具体的消费者消费对应的生产者。例如指定路由的关键字，并由它来绑定具体的队列与指定的生产者（或消费者）。路由的支持提供了消息传递与处理的灵活性，也有利于提高整个系统的消息处理能力。同时，路由对象有效地封装了寻找与匹配消息路径的逻辑，就好似一个调停者（Meditator），负责协调消息、队列与路径寻址之间关系。</p>

<p>除了以上的模式之外，Messaging模式提供了一个通信基础架构，使得我们可以将独立开发的服务整合到一个完整的系统中。 Message Translator模式则完成对消息的解析，使得不同的消息通道能够接收和识别不同格式的消息。而且通过引入这样的对象，也能够很好地避免出现盘根错节，彼此依赖的多个服务。Message Bus模式可以为企业提供一个面向服务的体系架构。它可以完成对消息的传递，对服务的适配与协调管理，并要求这些服务以统一的方式完成协作。</p>

<h2>2 消息模式的应用场景</h2>

<p>基于消息的分布式架构总是围绕着消息来做文章。例如可以将消息封装为对象，或者指定消息的规范例如SOAP，或者对实体对象的序列化与反序列化。这些方式的目的只有一个，就是将消息设计为生产者和消费者都能够明白的格式，并能通过消息通道进行传递。</p>

<h3>场景一：基于消息的统一服务架构</h3>

<p>在制造工业的CIMS系统中，我们尝试将各种业务以服务的形式公开给客户端的调用者，例如定义这样的接口：
{% codeblock lang:csharp %}
public interface IService {</p>

<pre><code>IMessage Execute(IMessage aMessage);
void SendRequest(IMessage aMessage);
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>之所以能够设计这样的服务，原因在于我们对业务信息进行了高度的抽象，以消息的形式在服务之间传递。此时的消息其实是生产者与消费者之间的契约或接口，只要遵循这样的契约，按照规定的格式对消息进行转换与抽取，就能很好地支持系统的分布式处理。</p>

<p>在这个CIMS系统中，我们将消息划分为ID，Name和Body，通过定义如下的接口方法，可以获得消息主体的相关属性：
{% codeblock lang:csharp %}
public interface IMessage:ICloneable
{</p>

<pre><code> string MessageID { get; set; }
 string MessageName() { get; set; }
 IMessageItemSequence CreateMessageBody();
 IMessageItemSequence GetMessageBody();
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>消息主体类Message实现了IMessage接口。在该类中，消息体Body为IMessageItemSequence类型。这个类型用于获取和设置消息的内容：Value和Item：
{% codeblock lang:csharp %}
public interface IItemValueSetting {</p>

<pre><code> string getSubValue(string name);
 void setSubValue(string name, string value);  
</code></pre>

<p>}
public interface IMessageItemSequence:IItemValueSetting, ICloneable
{</p>

<pre><code> IMessageItem GetMessageItem(string aName);
 IMessageItem CreateMessageItem(string aName);       
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>Value为字符串类型，它利用了HashTable存储Key和Value的键值对。Item则为IMessageItem类型，在IMessageItemSequence的实现类中，同样利用了HashTable存储Key和Item的键值对。</p>

<p>IMessageItem支持消息体的嵌套。它包含了两部分：SubValue和SubItem。实现的方式和IMessageItemSequence相似。通过定义这样的嵌套结构，使得消息的扩展成为可能。一般的消息结构如下所示：</p>

<p>各个消息对象之间的关系如图6所示：
{% img center /images/2012/12/message6.png 600 402 %}</p>

<center>图6 消息对象之间的关系</center>


<p>在实现服务进程通信之前，我们必须定义好各个服务或各个业务的消息格式。通过消息体的方法在服务的一端设置消息的值，然后发送，并在服务的另一端获得这些值。例如发送消息端定义如下的消息体：
{% codeblock lang:csharp %}
IMessageFactory factory = new MessageFactory();
IMessage message = factory.CreateMessage();
message.SetMessageName("service1");</p>

<p>IMessageItemSequence body = message.CreateMessageBody();
body.SetSubValue("subname1","subvalue1");
body.SetSubValue("subname2","subvalue2");</p>

<p>IMessageItem item1 = body.CreateMessageItem(”item1”);
item1.SetSubValue("subsubname11","subsubvalue11");
item1.SetSubValue("subsubname12","subsubvalue12");</p>

<p>//Send Request Message
MyServiceClient service = new MyServiceClient("Client");
service.SendRequest(message);
{% endcodeblock %}</p>

<p>我们在客户端引入了一个ServiceLocator对象，它通过MessageQueueListener对消息队列进行侦听，一旦接收到消息，就获取该消息中的name去定位它所对应的服务，然后调用服务的Execute(aMessage)方法，执行相关的业务。</p>

<p>ServiceLocator承担的定位职责其实是对存储在ServiceContainer容器中的服务进行查询。ServiceContainer容器可以读取配置文件，在启动服务的时候初始化所有的分布式服务（注意，这些服务都是无状态的），并对这些服务进行管理。它封装了服务的基本信息，诸如服务所在的位置，服务的部署方式等，从而避免服务的调用者直接依赖于服务的细节，既减轻了调用者的负担，还能够较好地实现服务的扩展与迁移。</p>

<p>在这个系统中，我们主要引入了Messaging模式，通过定义的IMessage接口，使得我们更好地对服务进行抽象，并以一种扁平的格式存储数据信息，从而解除服务之间的耦合。只要各个服务就共用的消息格式达成一致，请求者就可以不依赖于接收者的具体接口。通过引入的Message对象，我们就可以建立一种在行业中通用的消息模型与分布式服务模型。事实上，基于这样的一个框架与平台，在对制造行业的业务进行开发时，开发人员最主要的活动是与领域专家就各种业务的消息格式进行讨论，这样一种面向领域的消息语言，很好地扫清了技术人员与业务人员的沟通障碍；同时在各个子系统之间，我们也只需要维护服务间相互传递的消息接口表。每个服务的实现都是完全隔离的，有效地做到了对业务知识与基础设施的合理封装与隔离。</p>

<p>对于消息的格式和内容，我们考虑引入了Message Translator模式，负责对前面定义的消息结构进行翻译和解析。为了进一步减轻开发人员的负担，我们还可以基于该平台搭建一个消息-对象-关系的映射框架，引入实体引擎（Entity Engine）将消息转换为领域实体，使得服务的开发者能够以完全面向对象的思想开发各个服务组件，并通过调用持久层实现消息数据的持久化。同时，利用消息总线（此时的消息总线可以看做是各个服务组件的连接器）连接不同的服务，并允许异步地传递消息，对消息进行编码。这样一个基于消息的分布式架构如图7所示：</p>

<p>{% img center /images/2012/12/message7.png 600 449 %}</p>

<center>图7 基于Message Bus的CIMS分布式架构</center>


<h3>场景二：消息中间件的架构决策</h3>

<p>在一个医疗卫生系统中，我们面临了客户对系统性能/可用性的非功能需求。在我们最初启动该项目时，客户就表达了对性能与可用性的特别关注。客户希望最终用户在进行复杂的替换删除操作时，能够具有很好的用户体验，简言之，就是希望能够快速地得到操作的响应。问题在于这样的替换删除操作需要处理比较复杂的业务逻辑，同时牵涉到的关联数据量非常大，整个操作若需完成，最坏情况下可能需要几分钟的时间。我们可以通过引入缓存、索引、分页等多种方式对数据库操作进行性能调优，但整个操作的耗时始终无法达到客户的要求。由于该系统是在一个遗留系统的基础上开发，如果要引入Map-Reduce来处理这些操作，以满足质量需求，则对架构的影响太大，且不能很好地重用之前系统的某些组件。显然，付出的成本与收益并不成正比。</p>

<p>通过对需求进行分析，我们注意到最终客户并不需要实时获得结果，只要能够保证最终结果的一致性和完整性即可。关键在于就用户体验而言，他们不希望经历漫长的等待，然后再通知他们操作究竟是成功还是失败。这是一个典型需要通过后台任务进行异步处理的场景。</p>

<p>在企业应用系统中，我们常常会遭遇这样的场景。我们曾经在一个金融系统中尝试通过自己编写任务的方式来控制后台线程的并发访问，并完成对任务的调度。事实证明，这样的设计并非行之有效。对于这种典型的异步处理来说，基于消息传递的架构模式才是解决这一问题的最佳办法。</p>

<p>因为消息中间件的逐步成熟，对于这一问题的架构设计，已经由原来对设计实现的关注转为如何进行产品选型和技术决策。例如，在.NET平台下，架构师需要重点考虑的是应该选择哪种消息中间件来处理此等问题？这就需要我们必须结合具体的业务场景，来识别这种异步处理方式的风险，然后再根据这些风险去比较各种技术，以求寻找到最适合的方案。</p>

<p>通过分析业务场景以及客户性质，我们发现该业务场景具有如下特征：</p>

<pre><code>在一些特定情形下，可能会集中发生批量的替换删除操作，使得操作的并发量达到高峰；例如FDA要求召回一些违规药品时，就需要删除药品库中该药品的信息；

操作结果不要求实时性，但需要保证操作的可靠性，不能因为异常失败而导致某些操作无法进行；

自动操作的过程是不可逆转的，因此需要记录操作历史；

基于性能考虑，大多数操作需要调用数据库的存储过程；

操作的数据需要具备一定的安全性，避免被非法用户对数据造成破坏；

与操作相关的功能以组件形式封装，保证组件的可重用性、可扩展性与可测试性；

数据量可能随着最终用户的增多而逐渐增大；
</code></pre>

<p>针对如上的业务需求，我们决定从以下几个方面对各种技术方案进行横向的比较与考量。</p>

<pre><code>并发：选择的消息队列一定要很好地支持用户访问的并发性；
安全：消息队列是否提供了足够的安全机制；

性能伸缩：不能让消息队列成为整个系统的单一性能瓶颈；

部署：尽可能让消息队列的部署更为容易；

灾备：不能因为意外的错误、故障或其他因素导致处理数据的丢失；

API易用性：处理消息的API必须足够简单、并能够很好地支持测试与扩展；
我们先后考察了MSMQ、Resque、ActiveMQ和RabbitMQ，通过查询相关资料，以及编写Spike代码验证相关质量，我们最终选择了RabbitMQ。
</code></pre>

<p>我们选择放弃MSMQ，是因为它严重依赖Windows操作系统；它虽然提供了易用的GUI方便管理人员对其进行安装和部署，但若要编写自动化部署脚本，却非常困难。同时，MSMQ的队列容量不能查过4M字节，这也是我们无法接收的。Resque的问题是目前仅支持Ruby的客户端调用，不能很好地与.NET平台集成。此外，Resque对消息持久化的处理方式是写入到Redis中，因而需要在已有RDBMS的前提下，引入新的Storage。我们比较倾心于ActiveMQ与RabbitMQ，但通过编写测试代码，采用循环发送大数据消息以验证消息中间件的性能与稳定性时，我们发现ActiveMQ的表现并不太让人满意。至少，在我们的询证调研过程中，ActiveMQ会因为频繁发送大数据消息而偶尔出现崩溃的情况。相对而言，RabbitMQ在各个方面都比较适合我们的架构要求。</p>

<p>例如在灾备与稳定性方面，RabbitMQ提供了可持久化的队列，能够在队列服务崩溃的时候，将未处理的消息持久化到磁盘上。为了避免因为发送消息到写入消息之间的延迟导致信息丢失，RabbitMQ引入了Publisher Confirm机制以确保消息被真正地写入到磁盘中。它对Cluster的支持提供了Active/Passive与Active/Active两种模式。例如，在Active/Passive模式下，一旦一个节点失败，Passive节点就会马上被激活，并迅速替代失败的Active节点，承担起消息传递的职责。如图8所示：
{% img center /images/2012/12/message8.png 600 430 %}</p>

<center>图8 Active/Passive Cluster（图片来自RabbitMQ官方网站）</center>


<p>在并发处理方面，RabbitMQ本身是基于erlang编写的消息中间件，作为一门面向并发处理的编程语言，erlang对并发处理的天生优势使得我们对RabbitMQ的并发特性抱有信心。RabbitMQ可以非常容易地部署到Windows、Linux等操作系统下，同时，它也可以很好地部署到服务器集群中。它的队列容量是没有限制的（取决于安装RabbitMQ的磁盘容量），发送与接收信息的性能表现也非常好。RabbitMQ提供了Java、.NET、Erlang以及C语言的客户端API，调用非常简单，并且不会给整个系统引入太多第三方库的依赖。 例如.NET客户端只需要依赖一个程序集。</p>

<p>即使我们选择了RabbitMQ，但仍有必要对系统与具体的消息中间件进行解耦，这就要求我们对消息的生产者与消费者进行抽象，例如定义如下的接口：
{% codeblock lang:csharp %}</p>

<pre><code>public interface IQueueSubscriber
{
    void ListenTo&lt;T&gt;(string queueName, Action&lt;T&gt; action);
    void ListenTo&lt;T&gt;(string queueName, Predicate&lt;T&gt; messageProcessedSuccessfully);
    void ListenTo&lt;T&gt;(string queueName, Predicate&lt;T&gt; messageProcessedSuccessfully, bool requeueFailedMessages);
}

public interface IQueueProvider
{
    T Pop&lt;T&gt;(string queueName);
    T PopAndAwaitAcknowledgement&lt;T&gt;(string queueName, Predicate&lt;T&gt; messageProcessedSuccessfully);
    T PopAndAwaitAcknowledgement&lt;T&gt;(string queueName, Predicate&lt;T&gt; messageProcessedSuccessfully, bool requeueFailedMessages);
    void Push(FunctionalArea functionalArea, string routingKey, object payload);
}
</code></pre>

<p>{% endcodeblock %}</p>

<p>在这两个接口的实现类中，我们封装了RabbitMQ的调用类，例如：
{% codeblock lang:csharp %}</p>

<pre><code>public class RabbitMQSubscriber : IQueueSubscriber
{
    public void ListenTo&lt;T&gt;(string queueName, Action&lt;T&gt; action)
    {
        using (IConnection connection = _factory.OpenConnection())
        using (IModel channel = connection.CreateModel())
        {
            var consumer = new QueueingBasicConsumer(channel);
            string consumerTag = channel.BasicConsume(queueName, AcknowledgeImmediately, consumer);

            var response = (BasicDeliverEventArgs) consumer.Queue.Dequeue();
            var serializer = new JavaScriptSerializer();
            string json = Encoding.UTF8.GetString(response.Body);
            var message = serializer.Deserialize&lt;T&gt;(json);

            action(message);
        }
    }       
}
public class RabbitMQProvider : IQueueProvider
{

    public T Pop&lt;T&gt;(string queueName)
    {
        var returnVal = default(T);
        const bool acknowledgeImmediately = true;

        using (var connection = _factory.OpenConnection())
        using (var channel = connection.CreateModel())
        {
            var response = channel.BasicGet(queueName, acknowledgeImmediately);

            if (response != null)
            {
                var serializer = new JavaScriptSerializer();
                var json = Encoding.UTF8.GetString(response.Body);
                returnVal = serializer.Deserialize&lt;T&gt;(json);
            }
        }

        return returnVal;
    }
}
{% endcodeblock %}
</code></pre>

<p>我们用Quartz.Net来实现Batch Job。通过定义一个实现了IStatefulJob接口的Job类，在Execute()方法中完成对队列的侦听。Job中RabbitMQSubscriber类的ListenTo()方法会调用Queue的Dequeue()方法，当接收的消息到达队列时，Job会侦听到消息达到的事件，然后以同步的方式使得消息弹出队列，并将消息作为参数传递给Action委托。因此，在Batch Job的Execute()方法中，可以定义消息处理的方法，并调用RabbitMQSubscriber类的ListenTo()方法，如下所示（注意，这里传递的消息事实上是Job的Id）：
{% codeblock lang:csharp %}</p>

<pre><code>    public void Execute(JobExecutionContext context)
    {
        string queueName = queueConfigurer.GetQueueProviders().Queue.Name;
        try
        {
            queueSubscriber.ListenTo&lt;MyJob&gt;(
</code></pre>

<p>queueName,</p>

<pre><code>                job =&gt; request.MakeRequest(job.Id.ToString()));
        }
        catch(Exception err)
        {
            Log.WarnFormat("Unexpected exception while processing queue '{0}', Details: {1}", queueName, err);
        }
    }
</code></pre>

<p>{% endcodeblock %}</p>

<p>队列的相关信息例如队列名都存储在配置文件中。Execute()方法调用了request对象的MakeRequest()方法，并将获得的消息（即JobId）传递给该方法。它会根据JobId到数据库中查询该Job对应的信息，并执行真正的业务处理。</p>

<p>在对基于消息处理的架构进行决策时，除了前面提到的考虑因素外，还需要就许多设计细节进行多方位的判断与权衡。例如针对Job的执行以及队列的管理，就需要考虑如下因素：</p>

<pre><code>• 对Queue中Job状态的监控与查询；

• 对Job优先级的管理；

• 能否取消或终止执行时间过长的Job；

• 是否能够设定Job的执行时间；

• 是否能够设定Poll的间隔时间；

• 能否跨机器分布式的放入Job；

• 对失败Job的处理；

• 能否支持多个队列，命名队列；

• 能否允许执行Job的工作进程对应特定的队列；

• 对Dead Message的支持。
</code></pre>

<h2>3 选择的时机</h2>

<p>究竟在什么时候，我们应该选择基于消息处理的分布式架构？根据我参与的多个企业应用系统的经验，窃以为需要满足如下几个条件：</p>

<p>• 对操作的实时性要求不高，而需要执行的任务极为耗时；</p>

<p>• 存在企业内部的异构系统间的整合；</p>

<p>• 服务器资源需要合理分配与利用；</p>

<p>对于第一种情况，我们常常会选择消息队列来处理执行时间较长的任务。此时引入的消息队列就成了消息处理的缓冲区。消息队列引入的异步通信机制，使得发送方和接收方都不用等待对方返回成功消息，就可以继续执行下面的代码，从而提高了数据处理的能力。尤其是当访问量和数据流量较大的情况下，就可以结合消息队列与后台任务，通过避开高峰期对大数据进行处理，就可以有效降低数据库处理数据的负荷。前面提到的医疗卫生系统正是这样一种适用场景。</p>

<p>对于不同系统乃至于异构系统的整合，恰恰是消息模式善于处理的场景。只要规定了消息的格式与传递方式，就可以有效地实现不同系统之间的通信。在为某汽车制造商开发一个大型系统时，分销商作为.NET客户端，需要将数据传递到管理中心。这些数据将被Oracle的EBS（E-Business Suite）使用。分销商管理系统（Dealer Management System，DMS）采用了C/S结构，数据库为SQL Server，汽车制造商管理中心的EBS数据库为Oracle 10g。我们需要解决两种不同数据库间数据的传递。解决方案就是利用MSMQ，将数据转换为与数据库无关的消息数据，并在两端部署MSMQ服务器，建立消息队列以便于存储消息数据。实现架构如图9所示。
{% img center /images/2012/12/message9.png 600 448 %}</p>

<center>图9 利用MSMQ实现的分布式处理架构</center>


<p>首先，分销商的数据通过MSMQ传递到MSMQ Server，再将数据插入到SQL Server数据库的同时，利用FTP将数据传送到专门的文件服务器上。EBS App Server会将文件服务器中的文件，基于接口规范写入到Oracle数据库，从而实现.NET系统与Oracle系统之间的整合。</p>

<p>分布式系统通常能够缓解单个服务器的压力，通过将不同的业务操作与数据处理以不同的服务形式部署并运行在不同的服务器上，就可以有效地分配与利用服务器资源。在这种情况下，部署在不同服务器上的服务，既可能作为服务端，用以处理客户端调用的请求，也可能作为客户端，在处理完自己的业务后，将其余业务请求委派给其他服务。在早期的CORBA系统中，通过建立统一的Naming Service，用以管理和分派服务，并通过Event Service实现事件的分发与处理。但CORBA系统采用的是RPC的方式，需要将服务设计和部署为远程对象，并建立代理。如果通过消息通道的方式，则既可以解除这种对远程对象的依赖，又可以很好地支持异步调用模型。在前面提到的CIMS系统，就是通过消息总线提供消息传递的基础设施，并建立统一的消息处理服务模型，解除服务见的依赖，使得各个服务能够独立地部署到不同服务器上。</p>

<h2>4 面临的困难</h2>

<p>由于消息模式自身的特殊性，我们在运用消息模式建立基于消息的分布式架构时，常常会面临许多困难。</p>

<p>首先是系统集成的问题。由于系统之间的通信靠消息进行传递，就必须保证消息的一致性，同时，还需要维护系统之间（主要是服务之间）接口的稳定性。一旦接口发生变化，就可能影响到该接口的所有调用者。即使服务通过接口进行了抽象，由于消息持有双方服务规定的业务数据，在一定程度上违背了封装的要义。换言之，生产与消费消息的双方都紧耦合于消息。消息的变化会直接影响到各个服务接口的实现类。然而，为了尽可能保证接口的抽象性，我们所要处理的消息都不是强类型的，这就使得我们在编译期间很难发现因为消息内容发生变更产生的错误。在我之前提到的汽车零售商管理系统就存在这样的问题。当时我负责的CRM模块需要同时与多个子系统进行通信，而每个子系统又是由不同的团队进行开发。团队之间因为沟通原因，常常未能及时地同步接口表。虽然各个子系统的单元测试和功能测试都已通过，但直到对CRM进行集成测试，才发现存在大量消息不匹配的集成问题，这些问题的起因都是因为消息的变更。</p>

<p>解决的方案是引入充分的集成测试，甚至是回归测试，并需要及时运行这些测试，以快速地获得反馈。我们可以将集成测试作为提交代码的验证们，要求每次提交代码都必须运行集成测试与指定的回归测试 。这正是持续集成的体现。通过在本地构建与远程构建运行集成测试与回归测试，有效地保证本地版本与集成后的版本不会因为消息的改变使得功能遭受破坏。一旦遭受破坏，也能够及时获得反馈，发现问题，即刻解决这些问题，而不是等到项目后期集中进行集成测试。</p>

<p>另一个问题是后台任务的非实时性带来的测试困难。由于后台任务是定期对消息队列中的消息进行处理，因而触发的时机是不可预测的 。对于这种情况，我们通常会同时运用两种方案，双管其下地解决问题。首先，我们会为系统引入一个同步实现功能的版本，并通过在配置文件中引入toggle的开关机制，随时可以在同步功能与异步功能之间进行切换。如果我们能够保证消息队列处理与后台任务执行的正确性，就可以设置为同步功能，这样就能快速而准确地对该任务所代表的功能进行测试，并及时收获反馈。同时，我们可以在持续集成服务器上建立一个专门的管道（pipeline），用以运行基于消息处理的异步版本。这个管道对应的任务可以通过手动执行，也可以对管道设置定时器，在指定时间执行（例如在凌晨两点执行一次，这样在第二天开始工作之前可以获得反馈）。我们需要为该管道准备特定的执行环境，并将后台任务的侦听与执行时间修改为可以接受的值。这样既能够及时了解功能是否正确，又能保证基于消息的系统是工作正常的。</p>

<p>当然，分布式系统还存在解析消息、网络传递的性能损耗。对于这些问题，需要架构师审慎地分析业务场景，正确地选择架构方案与架构模式。相比较本地系统而言，分布式系统的维护难度可能成倍递增。这既需要我们在进行架构决策与设计时，充分考虑系统架构的稳定性，同时还需要引入系统日志处理。更好的做法是为日志处理增加错误通知的功能，只要发生消息处理的错误信息，就通过邮件、短信等方式通知系统管理员，及时地处理错误。因为只有在发生错误的当时查询错误日志，才能够更好对问题进行定位。同时，还可以为系统引入Error Message Queue以及Dead Message Queue，以便于处理错误和异常情况。</p>

<p>对于分布式系统而言，还需要考虑服务执行结果的一致性，尤其是当某个业务需要多个服务参与到一个会话中时，一旦某个服务发生故障，就可能导致应用出现状态不一致的情况，因为只有所有参与者都成功执行了任务，才能视为完全成功。这就牵涉到分布式事务的问题，此时任务的执行就变成了事务型的：即任务必须是原子的，结果状态必须保持一致。在任务处理过程中，状态修改是彼此隔离的，成功的状态修改在整个事务执行过程中是持久的。这就是事务的ACID（Atomic，Consistent，Isolated与Durable）属性。</p>

<p>一种方案是引入分布式事务协调器，即DTC（Distributed Transaction Coordinator）,将事务分为两段式甚至三段式提交，要求整个事务的所有参与者以投票形式决定事务是完全成功还是失败。另一种方案是降低对结果一致性的要求。根据eBay的最佳实践，考虑到分布式事务的成本，获得分布式资源即时的一致性是不必要的，也是不现实的。在Randy Shoup的文章《<a href="http://www.infoq.com/cn/articles/ebay-scalability-best-practices">可伸缩性最佳实践：来自eBay的经验</a>》中提到了Eric Brewer的CAP公理：分布式系统的三项重要指标——一致性（Consistency）、可用性（Availability）和 分区耐受性（Partition-tolerance）——在任意时刻，只有两项能同时成立。我们应该根据不同的应用场景，权衡这三个要素。在不必要保证即时的一致性前提下，我们可以考虑合理地划分服务，尽量将可能作用在同一个事务范围的业务操作部署在同一个进程中，以避免分布式部署。如果确实需要多个分布式服务之间保持执行结果的一致，可以考虑引入数据核对，异步恢复事件或集中决算等手段。</p>
]]></content>
  </entry>
  
</feed>
