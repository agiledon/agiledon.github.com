<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: Hadoop | 逸言]]></title>
  <link href="http://agiledon.github.com/blog/tags/hadoop/atom.xml" rel="self"/>
  <link href="http://agiledon.github.com/"/>
  <updated>2014-10-28T16:13:34+08:00</updated>
  <id>http://agiledon.github.com/</id>
  <author>
    <name><![CDATA[张逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark概览]]></title>
    <link href="http://agiledon.github.com/blog/2014/04/05/spark-overview/"/>
    <updated>2014-04-05T09:39:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2014/04/05/spark-overview</id>
    <content type="html"><![CDATA[<p>Spark具有先进的DAG执行引擎，支持cyclic data flow和内存计算。因此，它的运行速度，在内存中是Hadoop MapReduce的100倍，在磁盘中是10倍。如下是对比图：
{% img center /images/2014/spark-logistic-regression.png %}</p>

<p>这样的性能指标，真的让人心动啊！</p>

<p>Spark的API更为简单，提供了80个High Level的操作，可以很好地支持并行应用。它的API支持Scala、Java和Python，并且可以支持交互式的运行Scala与Python。来看看Spark统计Word字数的程序：
{% codeblock lang:scala %}
file = spark.textFile("hdfs://...")</p>

<p>file.flatMap(line => line.split(" "))</p>

<pre><code>.map(word =&gt; (word, 1))
.reduceByKey(_ + _)
</code></pre>

<p>{% endcodeblock %}</p>

<p>看看Hadoop的Word Count例子，简直弱爆了，爆表的节奏啊：
{% codeblock lang:java %}
public class WordCount {
  public static class TokenizerMapper</p>

<pre><code>   extends Mapper&lt;Object, Text, Text, IntWritable&gt;{

private final static IntWritable one = new IntWritable(1);
private Text word = new Text();

public void map(Object key, Text value, Context context
                ) throws IOException, InterruptedException {
  StringTokenizer itr = new StringTokenizer(value.toString());
  while (itr.hasMoreTokens()) {
    word.set(itr.nextToken());
    context.write(word, one);
  }
}
</code></pre>

<p>  }</p>

<p>  public static class IntSumReducer</p>

<pre><code>   extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
private IntWritable result = new IntWritable();


public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                   Context context
                   ) throws IOException, InterruptedException {
  int sum = 0;
  for (IntWritable val : values) {
    sum += val.get();
  }
  result.set(sum);
  context.write(key, result);
}
</code></pre>

<p>  }</p>

<p>  public static void main(String[] args) throws Exception {</p>

<pre><code>Configuration conf = new Configuration();
String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
if (otherArgs.length != 2) {
  System.err.println("Usage: wordcount &lt;in&gt; &lt;out&gt;");
  System.exit(2);
}
Job job = new Job(conf, "word count");
job.setJarByClass(WordCount.class);
job.setMapperClass(TokenizerMapper.class);
job.setCombinerClass(IntSumReducer.class);
job.setReducerClass(IntSumReducer.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
System.exit(job.waitForCompletion(true) ? 0 : 1);
</code></pre>

<p>  }
}
{% endcodeblock %}</p>

<p>当然，Hadoop有自己的一套框架，为整个的大数据处理做支持，例如HIVE，例如HDFS。Spark也不逊色，也有自己的SQL框架支持，即Shark，此外还支持流处理、机器学习以及图运算：
{% img center /images/2014/spark-stack.png %}</p>

<p>Spark并没有自己的分布式存储方案。不过已经有了强悍的HDFS，同为Aparch旗下的Spark又何必再造一个差不多的轮子呢？所以Spark可以很好地与Hadoop集成。例如可以运行在Hadoop 2的YARN集群下，可以读取现有的Hadoop数据。当然，Spark自身也支持standadlone的部署，或者部署到EC2等云平台下。除了可以读取HDFS数据，它还可以读取HBase，Cassandra等NoSQL数据库。这扩大了Spark的适用范围。</p>

<p>目前的Spark官方发布还仅仅是0.9的孵化版本，这为它的商用造成一点点阻碍。针对一个新的大数据项目而言，是选用Spark，还是Hadoop，还真的难以抉择。当然，对于我们这种玩技术的，从来都是喜新厌旧，心里自然是偏向Spark了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce技巧]]></title>
    <link href="http://agiledon.github.com/blog/2013/03/19/hadoop-mapreduce-skillset/"/>
    <updated>2013-03-19T13:19:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2013/03/19/hadoop-mapreduce-skillset</id>
    <content type="html"><![CDATA[<p>{% img left /images/2013/03/mapreduce.jpg %}
我在使用Hadoop编写MapReduce程序时，遇到了一些问题，通过在Google上查询资料，并结合自己对Hadoop的理解，逐一解决了这些问题。</p>

<h4>自定义Writable</h4>

<p>Hadoop对MapReduce中Key与Value的类型是有要求的，简单说来，这些类型必须支持Hadoop的序列化。为了提高序列化的性能，Hadoop还为Java中常见的基本类型提供了相应地支持序列化的类型，如IntWritable，LongWritable，并为String类型提供了Text类型。不过，这些Hadoop内建的类型并不足以支持真实遇到的业务。此时，就需要自定义Writable类，使得它既能够作为Job的Key或者Value，又能体现业务逻辑。</p>

<p>假设我已经从豆瓣抓取了书籍的数据，包括书籍的Title以及读者定义的Tag，并以Json格式存储在文本文件中。现在我希望提取这些数据中我感兴趣的内容，例如指定书籍的Tag列表，包括Tag被标记的次数。这些数据可以作为向量，为后面的数据分析提供基础数据。对于Map，我希望读取Json文件，然后得到每本书的Title，以及对应的单个Tag信息。作为Map的输出，我希望是我自己定义的类型BookTag。<!--more-->它只包括Tag的名称和标记次数：
{% codeblock lang:java %}
public class BookTag implements Writable {</p>

<pre><code>private String name;
private int count;

public BookTag() {
    count = 0;
}

public BookTag(String name, int count) {
    this.name = name;
    this.count = count;
}

@Override
public void write(DataOutput dataOutput) throws IOException {
    if (dataOutput != null) {
        Text.writeString(dataOutput, name);
        dataOutput.writeInt(count);
    }
}

@Override
public void readFields(DataInput dataInput) throws IOException {
    if (dataInput != null) {
        name = Text.readString(dataInput);
        count = dataInput.readInt();
    }
}

public String getName() {
    return name;
}

public void setName(String name) {
    this.name = name;
}

public int getCount() {
    return count;
}

public void setCount(int count) {
    this.count = count;
}

@Override
public String toString() {
    return "BookTag{" +
            "name='" + name + '\'' +
            ", count=" + count +
            '}';
}
</code></pre>

<p>}
{% endcodeblock %}
注意，在write()与readFields()方法中，对于String类型的处理完全不同于Int、Long等类型，它需要调用Text的相关静态方法。</p>

<p>针对每本书，Map出来的结果可能包含重复的BookTag信息（指Tag Name相同）；而我需要得到每个Tag的标记总和，以作为数据分析的向量。因此，作为Reduce的输入，可以是&lt;Text, Iterable<BookTag>>，但输出则应该是合并了相同Tag信息的结果。为此，我引入了BookTags类，在其内部维持了一个BookTag的Map，它同样需要实现Writable。由于BookTags包含了一个集合类型，因此它的实现会略有不同：
{% codeblock lang:java %}
public class BookTags implements Writable {</p>

<pre><code>private Map&lt;String, BookTag&gt; tags = new HashMap&lt;String, BookTag&gt;();

@Override
public void write(DataOutput dataOutput) throws IOException {
    dataOutput.writeInt(tags.size());
    for (BookTag tag : tags.values()) {
        tag.write(dataOutput);
    }
}

@Override
public void readFields(DataInput dataInput) throws IOException {
    int size = dataInput.readInt();
    for (int i = 0; i &lt; size; i++) {
        BookTag tag = new BookTag();
        tag.readFields(dataInput);
        tags.put(tag.getName(), tag);
    }
}

public void add(BookTag tag) {
        String tagName = tag.getName();
        if (tags.containsKey(tagName)) {
            BookTag bookTag = tags.get(tagName);
            bookTag.setCount(bookTag.getCount() + tag.getCount());
        } else {
            tags.put(tagName, tag);
        }
}

@Override
public String toString() {
    StringBuilder resultTags = new StringBuilder();
    for (BookTag tag : tags.values()) {
        resultTags.append(tag.toString());
        resultTags.append("|");
    }
    return resultTags.toString();
}
</code></pre>

<p>}
{% endcodeblock %}
其实，针对这种嵌套了集合的自定义Writable类型，由于嵌套的类型同样实现了Writable接口，因而同样可以调用嵌套类型的write()与readFields()方法，唯一的区别是需要将集合的Size写入到DataOutput中，以便于在读取时可以遍历集合。这实际上是一种Composite模式。</p>

<h4>Iterable的奇怪行为</h4>

<p>我需要在reduce()方法中，遍历传入的Iterable<BookTag>，以便于对重复的Tag进行累加操作。在遍历该对象时，我发现了一个奇怪现象，即最终得到的每本书的Tag信息，全部变成了一样的内容。通过对Reduce Job进行调试，发现每当遍历到Iterable<BookTag>的下一个元素时，这个最新的值就会覆盖之前得到的对象，使其变成同一个对象。通过Google，我发现这个问题是Hadoop的奇怪行为，即Iterable对象的next()方法永远会返回同一个对象。解决办法就是在遍历时，创建一个新对象放到我们要存储的集合中，如下第5行代码所示：
{% codeblock lang:java %}</p>

<pre><code>public static class BookReduce extends Reducer&lt;Text, BookTag, Text, BookTags&gt; {
    public void reduce(Text key, Iterable&lt;BookTag&gt; values, Context context) throws IOException, InterruptedException {
        BookTags bookTags = new BookTags();
        for (BookTag tag : values) {
            bookTags.add(new BookTag(tag.getName(), tag.getCount()));
        }
        context.write(key, bookTags);
    }
}
</code></pre>

<p>{% endcodeblock lang:java %}</p>

<p>这里得到的一个经验是，在编写MapReduce程序时，通过调试可以帮助你快速地定位问题。调试时，可以在项目的根目录下建立input文件夹，将数据源文件放入到该文件夹中，然后在调试的参数中设置即可。</p>

<h4>如何进行单元测试</h4>

<p>我们同样可以给MapReduce Job编写单元测试。除了可以使用Mockito进行Mock之外，我认为MRUnit可以更好地完成对MapReduce任务的验证。MRUnit为Map与Reduce提供了对应的Driver，即MapDriver与ReduceDriver。在编写测试用例时，我们只需要为Driver指定Input与Output，然后执行Driver的runTest()方法，即可测试任务的执行是否符合预期。这种预期是针对output输出的结果而言。以WordCounter为例，编写的单元测试如下：
{% codeblock lang:java %}
public class WordCounterTest {</p>

<pre><code>private MapDriver&lt;LongWritable, Text, Text, IntWritable&gt; mapDriver;
private ReduceDriver&lt;Text, IntWritable, Text, IntWritable&gt; reduceDriver;

@Before
public void setUp() {
    WordCounter.Map tokenizerMapper = new WordCounter.Map();
    WordCounter.Reduce reducer = new WordCounter.Reduce();
    mapDriver = MapDriver.newMapDriver(tokenizerMapper);
    reduceDriver = ReduceDriver.newReduceDriver(reducer);
}

@Test
public void should_execute_tokenizer_map_job() throws IOException {
    mapDriver.withInput(new LongWritable(12), new Text("I am Bruce Bruce"));
    mapDriver.withOutput(new Text("I"), new IntWritable(1));
    mapDriver.withOutput(new Text("am"), new IntWritable(1));
    mapDriver.withOutput(new Text("Bruce"), new IntWritable(1));
    mapDriver.withOutput(new Text("Bruce"), new IntWritable(1));
    mapDriver.runTest();
}

@Test
public void should_execute_reduce_job() {
    List&lt;IntWritable&gt; values = new ArrayList&lt;IntWritable&gt;();
    values.add(new IntWritable(1));
    values.add(new IntWritable(3));

    reduceDriver.withInput(new Text("Bruce"), values);
    reduceDriver.withOutput(new Text("Bruce"), new IntWritable(4));
    reduceDriver.runTest();
}
</code></pre>

<p>}
{% endcodeblock %}</p>

<h4>Chaining Job</h4>

<p>通过利用Hadoop提供的ChainMapper与ChainReducer，可以较为容易地实现多个Map Job或Reduce Job的链接。例如，我们可以将WordCounter分解为Tokenizer与Upper Case两个Map任务，最后执行Reduce。遗憾的是，ChainMapper与ChainReducer似乎不支持新版本的API，它要链接的Map与Reduce必须派生自MapReduceBase，并实现对应的Mapper或Reducer接口(说明，下面的代码基本上来自于StackOverFlow的<a href="http://stackoverflow.com/a/10470437/1008310">一个帖子</a>)。
{% codeblock lang:java %}
public class ChainWordCounter extends Configured implements Tool {</p>

<pre><code>public static class Tokenizer extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {
        StringTokenizer tokenizer = new StringTokenizer(value.toString());
        while (tokenizer.hasMoreTokens()) {
            word.set(tokenizer.nextToken());
            output.collect(word, one);
        }
    }
}

public static class UpperCaser extends MapReduceBase implements Mapper&lt;Text, IntWritable, Text, IntWritable&gt; {
    public void map(Text key, IntWritable count, OutputCollector&lt;Text, IntWritable&gt; collector, Reporter reporter) throws IOException {
        collector.collect(new Text(key.toString().toUpperCase()), count);
    }
}

public static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    private IntWritable result = new IntWritable();

    @Override
    public void reduce(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; collector, Reporter reporter) throws IOException {
        int sum = 0;
        while (values.hasNext()) {
            sum += values.next().get();
        }

        result.set(sum);
        collector.collect(key, result);
    }
}

public int run(String[] args) throws Exception {
    JobConf jobConf = new JobConf(getConf(), ChainWordCounter.class);
    FileInputFormat.setInputPaths(jobConf, new Path(args[0]));

    FileInputFormat.setInputPaths(jobConf, new Path(args[0]));
    Path outputDir = new Path(args[1]);
    FileOutputFormat.setOutputPath(jobConf, outputDir);
    outputDir.getFileSystem(getConf()).delete(outputDir, true);

    JobConf tokenizerMapConf = new JobConf(false);
    ChainMapper.addMapper(jobConf, Tokenizer.class, LongWritable.class, Text.class, Text.class, IntWritable.class, true, tokenizerMapConf);

    JobConf upperCaserMapConf = new JobConf(false);
    ChainMapper.addMapper(jobConf, UpperCaser.class, Text.class, IntWritable.class, Text.class, IntWritable.class, true, upperCaserMapConf);

    JobConf reduceConf = new JobConf(false);
    ChainReducer.setReducer(jobConf, Reduce.class, Text.class, IntWritable.class, Text.class, IntWritable.class, true, reduceConf);

    JobClient.runJob(jobConf);
    return 0;
}

public static void main(String[] args) throws Exception {
    int ret = ToolRunner.run(new Configuration(), new ChainWordCounter(), args);
    System.exit(ret);
}
</code></pre>

<p>}
{% endcodeblock %}</p>

<p>不知道什么时候这种机制能够很好地支持新版的API。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HDFS的架构]]></title>
    <link href="http://agiledon.github.com/blog/2013/02/16/architecture-of-hdfs/"/>
    <updated>2013-02-16T15:33:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2013/02/16/architecture-of-hdfs</id>
    <content type="html"><![CDATA[<p>{% img left /images/2013/02/hdfs.jpg %}
HDFS(Hadoop Distributed File System)作为<a href="http://hadoop.apache.org">Hadoop</a>下的一个子项目，是目前使用极为广泛的分布式文件系统。它的设计目的是提供一个高容错，且能部署在廉价硬件的分布式系统；同时，它能支持高吞吐量，适合大规模数据集应用。这一目标可以看做是HDFS的架构目标。显然，这样的架构设计主要还是满足系统的质量属性，包括如何保证分布式存储的可靠性，如何很好地支持硬件的水平扩展，如何支持对大数据处理的高性能以及客户端请求的高吞吐量。所以，HDFS的架构设计颇有参考价值，在Hadoop的Apache官方网站上也给出了<a href="http://hadoop.apache.org/docs/current/hdfs_design.html">HDFS的架构指南</a>。在<a href="http://www.aosabook.org/en/index.html">The Architecture of Open Source Applications</a>卷I的第8章也详细介绍了HDFS的架构。</p>

<p>HDFS的高层设计看起来很简单，主要包含NameNode与DataNode，它们之间的通信，包括客户端与HDFS NameNode服务器的通信则基于TCP/IP。客户端通过一个可配置的TCP端口连接到NameNode，通过ClientProtocol协议与NameNode交互。而DataNode使用DatanodeProtocol协议与NameNode交互。一个远程过程调用(RPC)模型被抽象出来封装ClientProtocol和Datanodeprotocol协议。</p>

<!--more-->


<p>通常，一个HDFS Cluter由一个NameNode和多个DataNode组成，且在大多数情况下，会由一台专门的机器运行NameNode实例。下图是HDFS的High Level Architecture：
{% img center /images/2013/02/hdfs01.gif %}</p>

<div align="center">本图来自<a href="http://www.ibm.com/developerworks/library/wa-introhdfs/">IBM DeveloperWorks</a></div>


<p>注意，在这个架构图中，观察各节点之间的通信，容易造成一个误解是NameNode会直接与DataNode通信。实则不然。虽然，NameNode可以看做是DataNode的管理者甚至是仲裁者，但由于DataNode的数量通常很多，且都是分布式部署在不同的机器上，若NameNode需要主动发起对各个DataNode的请求，会导致NameNode的负载过大，且对于网络的要求也极高。因此，在设计上，NameNode不会主动发起RPC，而是响应来自客户端或Datanode的RPC请求。如果NameNode需要获得指定DataNode的信息，则是通过DataNode调用函数后的一个简单返回值。每个DataNode都会维护一个开放的Socket，以支持客户端代码或其他DataNode的读写请求。NameNode知道该Socket的Host与Port。</p>

<p>{% img left /images/2013/02/hdfs02.png %}
一个好的架构必然遵循了好的架构原则。HDFS架构有许多值得我们借鉴或参考的设计决策，其中它所遵循的架构原则，对HDFS满足架构目标起到了决定性的作用。这些原则包括：元数据与数据分离；主/从架构；一次写入多次读取；移动计算比移动数据更划算。</p>

<h4>元数据与数据分离</h4>

<p>这主要体现在NameNode与DataNode之分，这种分离是HDFS最关键的架构决策。这两种节点的分离，意味着关注点的分离。对于一个文件系统而言，文件本身的属性（即元数据）与文件所持有的数据属于两个不同的关注点。一个简单的例子是文件名的更改。如果不实现分离，针对一个属性的修改，就可能需要对数据块进行操作，这是不合理的。如果不分离这两种节点，也不利于文件系统的分布式部署，因为我们很难找到一个主入口点。显然，这一原则是与后面提到的主/从架构是一脉相承的。</p>

<p>NameNode负责维护文件系统的名字空间，任何对文件系统名字空间或属性的修改都将被NameNode记录下来。NameNode会负责执行与文件系统命名空间的操作，包括打开、关闭、重命名文件或目录。它同时还要负责决定数据块到DataNode的映射。从某种意义上讲，NameNode是所有HDFS元数据的仲裁者和资源库。</p>

<p>DataNode则负责响应文件系统客户端发出的读写请求，同时还将在NameNode的指导下负责执行数据库的创建、删除以及复制。</p>

<p>因为所有的用户数据都存放在DataNode中，而不会流过NameNode，就使得NameNode的负载变小，且更有利于为NameNode建立副本。</p>

<h4>主/从架构</h4>

<p>主从架构表现的是Component之间的关系，即由主组件控制从组件。在HDFS中，一个HDFS集群是由一个NameNode和一定数目的DataNode组成。NameNode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。集群中的DataNode一般是一个节点一个，负责管理它所在节点上的存储。</p>

<h4>一次写入多次读写</h4>

<p>一次写入多次读写，即Write Once Read Many，是HDFS针对文件访问采取的访问模型。HDFS中的文件只能写一次，且在任何时间只能有一个Writer。当文件被创建，接着写入数据，最后，一旦文件被关闭，就不能再修改。这种模型可以有效地保证数据一致性，且避免了复杂的并发同步处理，很好地支持了对数据访问的高吞吐量。</p>

<h4>移动计算比移动数据更划算</h4>

<p>移动计算比移动数据更划算，即moving computation is cheaper than moving data。对于数据运算而言，越靠近数据，执行运算的性能就越好，尤其是当数据量非常大的时候，更是如此。由于分布式文件系统的数据并不一定存储在一台机器上，就使得运算的数据常常与执行运算的位置不相同。如果直接去远程访问数据，可能需要发起多次网络请求，且传输数据的成本也相当客观。因此最好的方式是保证数据与运算离得最近。这就带来两种不同的策略。一种是移动数据，另一种是移动运算。显然，移动数据，尤其是大数据的成本非常之高。要让网络的消耗最低，并提高系统的吞吐量，最佳方式是将运算的执行移到离它要处理的数据更近的地方，而不是移动数据。</p>

<p>HDFS在改善吞吐量与数据访问性能上还做出了一个好的设计决策，就是数据块的Staging。当客户端创建文件时，并没有立即将其发送给NameNode，而是将文件数据存储到本地的临时文件中。这个操作是透明的，客户端不会觉察，也不必关心。文件的创建事实上是一个流数据的写，当临时文件累计的数据量超过一个数据块大小时，客户端才会联系NameNode。NameNode将文件名插入文件系统的层次结构中，并且分配一个数据块给它。然后返回Datanode的标识符和目标数据块给客户端。接着，客户端将这块数据从本地临时文件上传到指定的Datanode上。当文件关闭时，在临时文件中剩余的没有上传的数据也会传输到指定的Datanode上。然后客户端告诉Namenode文件已经关闭。此时Namenode才将文件创建操作提交到HDFS的文件系统。这个操作的大致时序图如下所示：
{% img center /images/2013/02/hdfs03.png %}</p>

<p>采用这种客户端缓存的方式，可以有效地减少网络请求，避免大数据的写入造成网络堵塞，进而提高网络吞吐量。</p>
]]></content>
  </entry>
  
</feed>
