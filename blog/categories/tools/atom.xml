<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Tools | 逸言]]></title>
  <link href="http://agiledon.github.com/blog/categories/tools/atom.xml" rel="self"/>
  <link href="http://agiledon.github.com/"/>
  <updated>2014-10-28T16:13:34+08:00</updated>
  <id>http://agiledon.github.com/</id>
  <author>
    <name><![CDATA[张逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark的硬件配置]]></title>
    <link href="http://agiledon.github.com/blog/2014/05/08/hardware-on-spark/"/>
    <updated>2014-05-08T09:19:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2014/05/08/hardware-on-spark</id>
    <content type="html"><![CDATA[<p>从MapReduce的兴起，就带来一种思路，就是希望通过大量廉价的机器来处理以前需要耗费昂贵资源的海量数据。这种方式事实上是一种架构的水平伸缩模式——真正的以量取胜。毕竟，以现在的硬件发展来看，CPU的核数、内存的容量以及海量存储硬盘，都慢慢变得低廉而高效。然而，对于商业应用的海量数据挖掘或分析来看，硬件成本依旧是开发商非常关注的。当然最好的结果是：既要马儿跑得快，还要马儿少吃草。</p>

<p>Spark相对于Hadoop的MapReduce而言，确乎要跑得迅捷许多。然而，Spark这种In-Memory的计算模式，是否在硬件资源尤其是内存资源的消耗上，要求更高呢？我既找不到这么多机器，也无法租用多台虚拟instance，再没法测评的情况下，只要寻求Spark的官方网站，又或者通过Google搜索。从Spark官方网站，Databricks公司Patrick Wendell的演讲以及Matei Zaharia的Spark论文，找到了一些关于Spark硬件配置的支撑数据。</p>

<h3>Spark与存储系统</h3>

<p>如果Spark使用HDFS作为存储系统，则可以有效地运用Spark的standalone mode cluster，让Spark与HDFS部署在同一台机器上。这种模式的部署非常简单，且读取文件的性能更高。当然，Spark对内存的使用是有要求的，需要合理分配它与HDFS的资源。因此，需要配置Spark和HDFS的环境变量，为各自的任务分配内存和CPU资源，避免相互之间的资源争用。</p>

<p>若HDFS的机器足够好，这种部署可以优先考虑。若数据处理的执行效率要求非常高，那么还是需要采用分离的部署模式，例如部署在Hadoop YARN集群上。</p>

<h3>Spark对磁盘的要求</h3>

<p>Spark是in memory的迭代式运算平台，因此它对磁盘的要求不高。Spark官方推荐为每个节点配置4-8块磁盘，且并不需要配置为RAID（即将磁盘作为单独的mount point）。然后，通过配置spark.local.dir来指定磁盘列表。</p>

<h3>Spark对内存的要求</h3>

<p>Spark虽然是in memory的运算平台，但从官方资料看，似乎本身对内存的要求并不是特别苛刻。官方网站只是要求内存在8GB之上即可（Impala要求机器配置在128GB）。当然，真正要高效处理，仍然是内存越大越好。若内存超过200GB，则需要当心，因为JVM对超过200GB的内存管理存在问题，需要特别的配置。</p>

<p>内存容量足够大，还得真正分给了Spark才行。Spark建议需要提供至少75%的内存空间分配给Spark，至于其余的内存空间，则分配给操作系统与buffer cache。这就需要部署Spark的机器足够干净。</p>

<p>考虑内存消耗问题，倘若我们要处理的数据仅仅是进行一次处理，用完即丢弃，就应该避免使用cache或persist，从而降低对内存的损耗。若确实需要将数据加载到内存中，而内存又不足以加载，则可以设置Storage Level。Spark提供了三种Storage Level：MEMORY_ONLY（这是默认值），MEMORY_AND_DISK，以及DISK_ONLY。</p>

<p>关于数据的持久化，Spark默认是持久化到内存中。但它也提供了三种持久化RDD的存储方式：</p>

<ul>
<li><p>in-memory storage as deserialized Java objects</p></li>
<li><p>in-memory storage as serialised data</p></li>
<li><p>on-disk storage</p></li>
</ul>


<p>第一种存储方式性能最优，第二种方式则对RDD的展现方式（Representing）提供了扩展，第三种方式则用于内存不足时。</p>

<p>然而，在最新版（V1.0.2）的Spark中，提供了更多的Storage Level选择。一个值得注意的选项是OFF_HEAP，它能够将RDD以序列化格式存储到Tachyon中。相比MEMORY_ONLY_SER，这一选项能够减少执行垃圾回收，使Spark的执行器（executor）更小，并能共享内存池。<a href="http://tachyon-project.org/">Tachyon</a>是一个基于内存的分布式文件系统，性能远超HDFS。Tachyon与Spark同源同宗，都烙有伯克利AMPLab的印记。目前，Tachyon的版本为0.5.0，还处于实验阶段。</p>

<p>注意，RDDs是Lazy的，在执行Transformation操作如map、filter时，并不会提交Job，只有在执行Action操作如count、first时，才会执行Job，此时才会进行数据的加载。当然，对于一些shuffle操作，例如reduceByKey，虽然仅是Transformation操作，但它在执行时会将一些中间数据进行持久化，而无需显式调用persist()函数。这是为了应对当节点出现故障时，能够避免针对大量数据进行重计算。要计算Spark加载的Dataset大小，可以通过Spark提供的Web UI Monitoring工具来帮助分析与判断。</p>

<p>Spark的RDD是具有分区（partition）的，Spark并非是将整个RDD一次性加载到内存中。Spark针对partition提供了eviction policy，这一Policy采用了LRU（Least Recently Used）机制。当一个新的RDD分区需要计算时，如果没有合适的空间存储，就会根据LRU策略，将最少访问的RDD分区弹出，除非这个新分区与最少访问的分区属于同一个RDD。这也在一定程度上缓和了对内存的消耗。</p>

<p>Spark对内存的消耗主要分为三部分：1. 数据集中对象的大小；2. 访问这些对象的内存消耗；3. 垃圾回收GC的消耗。一个通常的内存消耗计算方法是：内存消耗大小= 对象字段中原生数据 * (2~5)。 这是因为Spark运行在JVM之上，操作的Java对象都有定义的“object header”，而数据结构（如Map，LinkedList）对象自身也需要占用内存空间。此外，对于存储在数据结构中的基本类型，还需要装箱（Boxing）。Spark也提供了一些内存调优机制，例如执行对象的序列化，可以释放一部分内存空间。还可以通过为JVM设置flag来标记存放的字节数（选择4个字节而非8个字节）。在JDK 7下，还可以做更多优化，例如对字符编码的设置。这些配置都可以在spark-env.sh中设置。</p>

<h3>Spark对网络的要求</h3>

<p>Spark属于网络绑定型系统，因而建议使用10G及以上的网络带宽。</p>

<h3>Spark对CPU的要求</h3>

<p>Spark可以支持一台机器扩展至数十个CPU core，它实现的是线程之间最小共享。若内存足够大，则制约运算性能的就是网络带宽与CPU数。</p>

<p>Spark官方利用Amazon EC2的环境对Spark进行了基准测评。例如，在交互方式下进行数据挖掘（Interative Data Mining），租用Amazon EC2的100个实例，配置为8核、68GB的内存。对1TB的维基百科页面查阅日志（维基百科两年的数据）进行数据挖掘。在查询时，针对整个输入数据进行全扫描，只需要耗费5-7秒的时间。如下图所示：
{% img center /images/2014/sparkbenchmark.png %}</p>

<p>在Matei Zaharia的Spark论文中还给出了一些使用Spark的真实案例。视频处理公司Conviva，使用Spark将数据子集加载到RDD中。报道说明，对于200GB压缩过的数据进行查询和聚合操作，并运行在两台Spark机器上，占用内存为96GB，执行完全部操作需要耗费30分钟左右的时间。同比情况下，Hadoop需要耗费20小时。注意：之所以200GB的压缩数据只占用96GB内存，是因为RDD的处理方式，使得我们可以只加载匹配客户过滤的行和列，而非所有压缩数据。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git的使用场景]]></title>
    <link href="http://agiledon.github.com/blog/2013/12/24/the-scenarios-for-using-git/"/>
    <updated>2013-12-24T21:38:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2013/12/24/the-scenarios-for-using-git</id>
    <content type="html"><![CDATA[<p>无论学习什么技术，都需要了解该技术的本质。若是靠死记硬背该技术提供的方法或者语法，终归是知其然而不知其所以然，当发现错误时，你根本不知道是什么原因导致的。我在使用Git时，就处于这种知其然而不知其所以然的状态。现在，再来补补课。</p>

<p>Git有三个工作区域，分别为：工作目录（Working Directory）、暂存区（Stage或Index）以及资源库（Repository或Git Directory）。下图是文件在这三个工作区域之间的关系：
{% img center images/2013/lifecycle.png %}</p>

<p>参考Pro Git一书，它给出了Git的几个要点：
* 直接快照，而非比较差异：Git与其他版本管理系统的主要差别在于，Git只关心文件数据的整体是否发生了变化，而其他多数版本管理系统则只关心文件内容的具体差异。Git并不保存文件前后变化的差异数据，更像是把变化的文件做一个快照，然后记录在一个微型的文件系统中。每次提交更新时，会比较这个快照。若文件没有变化，Git则只对上次保存的快照作一个链接。你可以理解Git就是一个小型的文件系统。
* 近乎所有操作都可本地执行：无需多说，这本身就是分布式版本管理系统的特征。
* 时刻保持数据完整性：保存到Git前，所有数据都要进行内容的校验和（checksum），并将该结果作为数据的唯一标识。Git使用了SHA-1算法计算数据的校验和，并将该结果作为索引，而非文件名。
{% img center images/2013/git-hash.png %}
* 多数操作仅添加数据</p>

<!--more-->


<p>Pro Git一书认为任何一个文件在Git内部可以被分为三种状态：已提交（Committed）、已修改（Modified）和已暂存（Staged）。然而，这并不足以说明一个文件在不同的工作区域所展现的状态。我认为两种状态足以表达Git中的文件，即：未跟踪（Untracked）和已跟踪（Tracked）。而对于已跟踪状态，我又将其分为：未修改的（Unmodified），Modified（已修改的），暂存的（Staged）和已提交的（committed）。下图基本表达了我的思路：
{% img center images/2013/file-lifecycle.png %}</p>

<p>这个图表现了多种场景，满足了我们在使用Git时耳濡目染的操作情形。</p>

<h2>场景1：暂存文件以及取消已暂存的文件</h2>

<p>可以参考上图中上面部分黑色箭头标示。当我们通过git init在本地初始化了Git工作目录后，新增了一个README.txt文件时，此时该文件处于Untracked状态。接下来执行命令：
<code>
git add README.txt
</code></p>

<p>add命令可以暂存此文件，此时，状态变更为Staged状态，被放到了Git暂存区中。若我们要提交此文件到Git资源库，就可以执行git commit命令，文件状态变为committed。例如：
<code>
git commit -m "first commit"
</code></p>

<p>有时候，我们希望取消已暂存的文件。例如说，我在工作目录中增加了两个文件，然后暂存了它们。后来发现其中一个文件并不需要在Git中管理，希望能够取消暂存。由于此时的文件处于Staged状态，我们只需要删掉Stage中对此文件的跟踪即可。这时需要执行的命令是：
<code>
git rm --cached README.txt
</code></p>

<p><strong>注意：此时取消暂存的文件从来就不曾提交过，也即是说没有在Git Repository留下过它的身影。这时的取消暂存实则是删掉暂存的信息。与后面场景演示的取消暂存并不相同。</strong></p>

<h2>场景2：修改已提交文件以及取消已暂存的内容</h2>

<p>一旦文件被提交，就会在Git Repository形成提交记录（以hash作为键）。倘若我们此时push提交到远程Git服务器，Git服务器应与本地库保持一致。</p>

<p>现在，让我们看看图中红色箭头展现的流程。我们修改了已提交的README.txt文件，于是文件状态就变更为Modified。这部分修改的内容并没有被放入暂存区，若要提交此次修改，就还需要再次执行git add命令，将这次新的修改放入到暂存区。这个流程包括后面的提交都与场景1相似。唯一不同的是“取消已暂存的内容”。</p>

<p>虽然同样是取消暂存，但它与场景1是完全不同的概念。场景1实则是要取消暂存区的文件，因此使用了git rm --cached，本质上讲其实是删除。而这里的取消，其实是希望取消暂存区中已经被添加的修改内容，文件本身仍然保留在暂存区中。故而执行的命令为：
<code>
git reset HEAD README.txt
</code></p>

<p>HEAD是何意呢？在Git中，HEAD是一个特别的指针，指向你正在工作的本地分支。当前分支就是master。如下图所示：
{% img center images/2013/git_head.png %}</p>

<p>而reset命令的意思是重新设置当前的HEAD指针到特定的状态。由于当前的README.txt还没有提交到master分支的Repository中。因此，这条命令实则就是将HEAD指向README.txt文件在当前master分支的Repository状态，从而保证了对README.txt文件而言，暂存区与Repository的一致——取消了README.txt文件在暂存区的内容。</p>

<h2>场景3：修改文件以及撤销修改内容</h2>

<p>再看图中的绿色箭头与蓝色箭头展现的流程。我们不是初始化git工作目录，而是通过git clone从远程Repository克隆了项目，此时会在当前目录建立git工作目录。此时的文件全部处于Unmodified状态。</p>

<p>现在，我们修改文件，例如hello.java。一旦被修改，文件状态就迁移到Modified状态。倘若需要暂存此次修改，甚至提交到Git Repository，则执行的流程与场景1相同（如蓝色箭头线所示）。</p>

<p>然而，我们可能希望放弃此次修改，即不将修改的内容放入暂存区。这时，应执行checkout命令：
<code>
git checkout -- hello.java
</code></p>

<p>在执行checkout命令时要慎重。因为它要撤销的内容并没有被放入到暂存区或Repository。一旦撤销，就一去不复返了。</p>

<h2>概念区分：fetch vs. pull</h2>

<p>fetch命令只是将远端数据拉到本地仓库，并不自动合并到当前工作分支。若要合并，还需手动合并。例如，执行git fetch origin，就会抓取自上次克隆以来别人上传到此远程仓库中的所有更新。</p>

<p>pull命令则除了会抓取数据，还能将远端分支自动合并到本地仓库中当前分支。</p>

<h2>场景4：撤销提交</h2>

<p>在Git中若要撤销提交，可以使用reset或者revert命令。但二者有着显著的区别：
{% img left images/2013/git_revert_vs_reset.png %}
revert命令可以撤销已经提交的快照，但它并不会将该提交从项目的提交历史中移除，而是会判断要撤销的这次提交引入了哪些变化，然后将此变化撤销（此次撤销事实上还是一种变化），再将这次撤销作为一个提交。因此，在执行revert命令后，如果通过git log查看提交历史，可以看到会新增一个revert提交。命令为：
<code>
git revert &lt;commit&gt;
</code></p>

<p>这个commit可以是指定提交对应的hash code。我们也可以用HEAD指针：
<code>
git revert HEAD~n
</code></p>

<p>如果是revert当前提交，则不需要HEAD后的~n。</p>

<p>reset命令就字面意义已经表达了该操作的含义为“重置”。由于Git的提交记录是由HEAD指针指向当前分支。重置就是搬动这个指针到指定的snapshot。如果说revert是一种<strong>安全</strong>的撤销方式，则reset就是一种<strong>危险</strong>的撤销方式。默认情况下，如果使用reset命令，会将当前的分支回退到指定commit，然后自指定commit到最新commit之间的内容会放在工作目录下，使得我们可以再提交。这个命令为：
<code>
git reset &lt;commit&gt;
</code></p>

<p>与前相同，这个commit就是提交对应的hash code。同样，也可以使用HEAD指针。不过如果是撤销当前提交，与revert不同的是，需要指定为：HEAD~1。这是因为HEAD指针指向了当前提交。reset与revert的意义不一样。revert对应的commit为目标提交，意思为：“撤销目标提交”，因而git revert HEAD，代表的就是“将当前提交撤销”。而reset对应的commit表示将指针移向给定的Commit。如果执行git reset HEAD，代表的就是“将当前指针指向当前提交”，相当于没做任何操作。所以应该执行git reset HEAD~1。</p>

<p>如果确实要撤销操作，而前面的内容并不需要，在使用reset命令时，可以添加--hard参数：
<code>
git reset --hard &lt;commit&gt;
</code></p>

<p>**注意：针对远程的提交记录，应尽量避免使用git reset命令。倘若在本地进行了reset之后，又进行了另外的修改并提交。此时，本地的提交记录与远程的提交记录在reset的那个点产生了分叉。如下图所示：
{% img center images/2013/git_reset_commit.png %}</p>

<p>此时，如果执行git push，会在本地合并后提交，并同步远程提交记录。则团队其他成员会因为这个变化的提交记录而困惑。由于一部分变更消失，甚至可能导致一些数据被破坏。因此，使用reset命令要格外当心，通常情况，应尽量针对本地提交（未push到远程）进行reset。优先考虑使用revert命令。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Homebrew在Mountain Lion上安装MySQL]]></title>
    <link href="http://agiledon.github.com/blog/2013/01/06/install-mysql-on-mountain-lion-with-homebrew/"/>
    <updated>2013-01-06T13:05:00+08:00</updated>
    <id>http://agiledon.github.com/blog/2013/01/06/install-mysql-on-mountain-lion-with-homebrew</id>
    <content type="html"><![CDATA[<p>{% img center /images/2013/01/Mountain-Lion.jpg %}</p>

<p>使用Homebrew在Max OS下安装，一切都变得容易了许多。你只需要知道你要安装的软件或工具的名称，然后输入一条命令行即可。不过安装MySQL就有点不一样，在输入命令行安装了MySQL之后，还需要做一些配置。</p>

<p>首先运行命令：
{% codeblock %}
$ brew install mysql
{% endcodeblock %}</p>

<p>Homebrew确实安装了MySQL，但是运行mysql命令，却并不能进入MySQL。通过在Google上查询解决方案，找到博客<a href="http://madebyhoundstooth.com/blog/install-mysql-on-mountain-lion-with-homebrew/">Install MySQL on Mountain Lion with Homebrew</a>，给出了如下的步骤：
{% codeblock %}
$ mkdir -p ~/Library/LaunchAgents
$ cp /usr/local/Cellar/mysql/5.5.25a/homebrew.mxcl.mysql.plist ~/Library/LaunchAgents/
$ launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist</p>

<p>$ unset TMPDIR
$ mysql_install_db --verbose --user=<code>YOUR_USER_NAME</code> --basedir="$(brew --prefix mysql)" --datadir=/usr/local/var/mysql --tmpdir=/tmp</p>

<p>$ /usr/local/Cellar/mysql/5.5.25a/bin/mysqladmin -u root password 'YOUR_NEW_PASSWORD'</p>

<p>$ mysql.server start
{% endcodeblock %}</p>

<p>现在，你就可以运行mysql命令使用MySQL了。</p>
]]></content>
  </entry>
  
</feed>
